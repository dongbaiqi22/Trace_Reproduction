{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:22.075493Z",
     "start_time": "2025-12-12T20:17:21.162573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"/Users/dongbaiqi/Desktop/Tang Lab/Spatially_aware_dimensionality_reduction/PythonProject/data/fish_data.npz\")\n",
    "spks = data['spks']\n",
    "xyz = data['xyz']"
   ],
   "id": "bc01f13bc4e71bbb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:19:09.305622Z",
     "start_time": "2025-09-25T23:19:09.304270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trial 1\n",
    "# print(data['stims'][20:480])"
   ],
   "id": "2ffa9b1974fa3fdf",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:19:09.320551Z",
     "start_time": "2025-09-25T23:19:09.319197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trial 2\n",
    "# print(data['stims'][500:960])"
   ],
   "id": "5082b05a53e1f734",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:19:09.332852Z",
     "start_time": "2025-09-25T23:19:09.331466Z"
    }
   },
   "cell_type": "code",
   "source": "# print(data['stims'][2690:3150])",
   "id": "5bae79b12b03ad10",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:19:09.345229Z",
     "start_time": "2025-09-25T23:19:09.343855Z"
    }
   },
   "cell_type": "code",
   "source": "# print(data['stims'][3170:3630])",
   "id": "f4d449402989eb44",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:19:09.357273Z",
     "start_time": "2025-09-25T23:19:09.355986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trial 3\n",
    "# print(data['stims'][5240: 5700])"
   ],
   "id": "87f4614721ca27e7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:19:09.369194Z",
     "start_time": "2025-09-25T23:19:09.367855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trial 4\n",
    "# print(data['stims'][5720: 6180])"
   ],
   "id": "7f8fc526a437118a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:26.914039Z",
     "start_time": "2025-12-12T20:17:26.702983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_neurons = spks.shape[0]\n",
    "np.random.seed(42)\n",
    "selected_idx = np.random.choice(n_neurons, size=1000, replace=False)\n",
    "\n",
    "X_all      = spks[selected_idx, :]\n",
    "coords_np      = xyz[selected_idx, :]"
   ],
   "id": "1332281672bd1836",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:27.441802Z",
     "start_time": "2025-12-12T20:17:27.435049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trial1 = X_all[:, 20:480]\n",
    "print(\"trial 1 shape: \", trial1.shape)\n",
    "trial2 = X_all[:, 500:960]\n",
    "print(\"trial 2 shape: \", trial2.shape)\n",
    "trial3 = X_all[:, 5240: 5700]\n",
    "print(\"trial 3 shape: \", trial3.shape)\n",
    "trial4 = X_all[:, 5720: 6180]\n",
    "print(\"trial 4 shape: \", trial4.shape)\n",
    "\n",
    "X_cat = np.concatenate((trial1, trial2, trial3, trial4), axis=1)\n",
    "print(X_cat.shape)"
   ],
   "id": "57eedd014d24fadc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 1 shape:  (1000, 460)\n",
      "trial 2 shape:  (1000, 460)\n",
      "trial 3 shape:  (1000, 460)\n",
      "trial 4 shape:  (1000, 460)\n",
      "(1000, 1840)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:28.369763Z",
     "start_time": "2025-12-12T20:17:28.344623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity_matrix(X):\n",
    "    X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)\n",
    "    return np.dot(X_norm, X_norm.T)\n",
    "\n",
    "\n",
    "func_mat_np = cosine_similarity_matrix(X_cat)"
   ],
   "id": "565be28096957223",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:30.102485Z",
     "start_time": "2025-12-12T20:17:29.494694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# func_mat inspection to choose threshold\n",
    "# use 0.2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stat = func_mat_np >= 0.3\n",
    "stat = stat.sum(axis=1)\n",
    "plt.figure()\n",
    "plt.hist(stat, bins=20, edgecolor='black')\n",
    "plt.xlabel(\"Number of similarities >= threshold\")\n",
    "plt.ylabel(\"Count of rows\")\n",
    "plt.title(\"Histogram of row counts (similarity >= threshold)\")\n",
    "plt.show()"
   ],
   "id": "184cba7c3a20f6d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:32.399318Z",
     "start_time": "2025-12-12T20:17:31.420298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# distance inspection to choose threshold\n",
    "\n",
    "# Use 70\n",
    "from scipy.spatial.distance import cdist\n",
    "dist = cdist(coords_np, coords_np, metric='euclidean')\n",
    "dist_stat = 10000 * np.eye(1000) + dist\n",
    "min = dist_stat.min(axis=0)\n",
    "print(min.shape)\n",
    "print(min.max())\n",
    "\n",
    "dist_stat = dist <= 40\n",
    "dist_stat = dist_stat.sum(axis=1)\n",
    "plt.figure()\n",
    "plt.hist(dist_stat, bins=20, edgecolor='black')\n",
    "plt.xlabel(\"Number of distance <= threshold\")\n",
    "plt.ylabel(\"Count of rows\")\n",
    "plt.title(\"Histogram of row counts (distance >= threshold)\")\n",
    "plt.show()"
   ],
   "id": "ab6f9e9345aa25ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "43.64630568559039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:17:33.999982Z",
     "start_time": "2025-12-12T20:17:33.997196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = X_cat.shape[0]\n",
    "T = X_cat.shape[1]\n",
    "\n",
    "X_train = X_cat.reshape(N, 4, T//4)\n",
    "print(X_train.shape)"
   ],
   "id": "83f9c4a511f77691",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4, 460)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:20:18.346745Z",
     "start_time": "2025-12-12T20:17:39.949183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from model.train import train_trace_with_custom_pairs\n",
    "\n",
    "# ============ 0) data preparation ============\n",
    "def to_float_tensor(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.from_numpy(x).float()\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return x.float()\n",
    "    else:\n",
    "        raise TypeError(\"X_train / X_test must be numpy.ndarray or torch.Tensor\")\n",
    "\n",
    "X_train_t = to_float_tensor(X_train)  # [N, R, T]\n",
    "\n",
    "N, R, T = X_train_t.shape\n",
    "print(f\"[info] X_train: N={N}, R={R}, T={T}\")\n",
    "\n",
    "# random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[info] device:\", device)\n",
    "\n",
    "# ============ 1) spatial/functional prior ============\n",
    "# coords   = None         # torch.Tensor [N,2] or [N,3]\n",
    "func_vec = None         # torch.Tensor [N]\n",
    "# func_mat = None         # torch.Tensor [N,N]\n",
    "\n",
    "# uncomment if you have\n",
    "coords   = torch.from_numpy(coords_np).float()\n",
    "# func_vec = torch.from_numpy(func_vec_np).float()\n",
    "func_mat = torch.from_numpy(func_mat_np).float()\n",
    "\n",
    "# ============ 2) training ============\n",
    "k = max(1, R // 2)\n",
    "\n",
    "epochs     = 1500\n",
    "batch_size = 200\n",
    "lr         = 1e-3\n",
    "proj_mode  = \"large\"\n",
    "\n",
    "model, history = train_trace_with_custom_pairs(\n",
    "    X=X_train_t,\n",
    "    T=T,\n",
    "    #coords=coords,           # None or [N,2/3]\n",
    "    coords=None,\n",
    "    func_vec=None,       # None or [N]\n",
    "    #func_mat=func_mat,       # None or [N,N]\n",
    "    func_mat=None,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    k=k,\n",
    "    r_max=100,              # spatial constraint\n",
    "    f_pos_th=0.5,            # functional threshold\n",
    "    same_sign=True,          # this is only func_vec\n",
    "    proj_mode=proj_mode,\n",
    "    device=device,\n",
    "    grad_clip=5.0,\n",
    "    log_every=5,\n",
    "    return_history=True,\n",
    ")\n",
    "\n",
    "print(f\"[done] training finished. logged {len(history['loss'])} loss points.\")"
   ],
   "id": "cf9ea16f6e834c93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] X_train: N=1000, R=4, T=460\n",
      "[info] device: cpu\n",
      "[warn] coords/func is not provided, only depends TRACE's positive pair of two views\n",
      "[epoch 001] mean loss = 5.834108 (batches=5)\n",
      "[epoch 002] mean loss = 5.538480 (batches=5)\n",
      "[epoch 003] mean loss = 5.300674 (batches=5)\n",
      "[epoch 004] mean loss = 5.142929 (batches=5)\n",
      "[epoch 005] mean loss = 5.066781 (batches=5)\n",
      "[epoch 006] mean loss = 4.984436 (batches=5)\n",
      "[epoch 007] mean loss = 4.951188 (batches=5)\n",
      "[epoch 008] mean loss = 4.822831 (batches=5)\n",
      "[epoch 009] mean loss = 4.752832 (batches=5)\n",
      "[epoch 010] mean loss = 4.635893 (batches=5)\n",
      "[epoch 011] mean loss = 4.585636 (batches=5)\n",
      "[epoch 012] mean loss = 4.640332 (batches=5)\n",
      "[epoch 013] mean loss = 4.579754 (batches=5)\n",
      "[epoch 014] mean loss = 4.476997 (batches=5)\n",
      "[epoch 015] mean loss = 4.530504 (batches=5)\n",
      "[epoch 016] mean loss = 4.444837 (batches=5)\n",
      "[epoch 017] mean loss = 4.422685 (batches=5)\n",
      "[epoch 018] mean loss = 4.312800 (batches=5)\n",
      "[epoch 019] mean loss = 4.270968 (batches=5)\n",
      "[epoch 020] mean loss = 4.147928 (batches=5)\n",
      "[epoch 021] mean loss = 4.178699 (batches=5)\n",
      "[epoch 022] mean loss = 4.208937 (batches=5)\n",
      "[epoch 023] mean loss = 4.144151 (batches=5)\n",
      "[epoch 024] mean loss = 4.142946 (batches=5)\n",
      "[epoch 025] mean loss = 4.067958 (batches=5)\n",
      "[epoch 026] mean loss = 3.979672 (batches=5)\n",
      "[epoch 027] mean loss = 3.995675 (batches=5)\n",
      "[epoch 028] mean loss = 3.898892 (batches=5)\n",
      "[epoch 029] mean loss = 3.831189 (batches=5)\n",
      "[epoch 030] mean loss = 3.846200 (batches=5)\n",
      "[epoch 031] mean loss = 3.842304 (batches=5)\n",
      "[epoch 032] mean loss = 3.839959 (batches=5)\n",
      "[epoch 033] mean loss = 3.851083 (batches=5)\n",
      "[epoch 034] mean loss = 3.758267 (batches=5)\n",
      "[epoch 035] mean loss = 3.799999 (batches=5)\n",
      "[epoch 036] mean loss = 3.663648 (batches=5)\n",
      "[epoch 037] mean loss = 3.577750 (batches=5)\n",
      "[epoch 038] mean loss = 3.523558 (batches=5)\n",
      "[epoch 039] mean loss = 3.610285 (batches=5)\n",
      "[epoch 040] mean loss = 3.583037 (batches=5)\n",
      "[epoch 041] mean loss = 3.577308 (batches=5)\n",
      "[epoch 042] mean loss = 3.542772 (batches=5)\n",
      "[epoch 043] mean loss = 3.405158 (batches=5)\n",
      "[epoch 044] mean loss = 3.447977 (batches=5)\n",
      "[epoch 045] mean loss = 3.434297 (batches=5)\n",
      "[epoch 046] mean loss = 3.341359 (batches=5)\n",
      "[epoch 047] mean loss = 3.364313 (batches=5)\n",
      "[epoch 048] mean loss = 3.358709 (batches=5)\n",
      "[epoch 049] mean loss = 3.244961 (batches=5)\n",
      "[epoch 050] mean loss = 3.239092 (batches=5)\n",
      "[epoch 051] mean loss = 3.265636 (batches=5)\n",
      "[epoch 052] mean loss = 3.177620 (batches=5)\n",
      "[epoch 053] mean loss = 3.201089 (batches=5)\n",
      "[epoch 054] mean loss = 3.133102 (batches=5)\n",
      "[epoch 055] mean loss = 3.159858 (batches=5)\n",
      "[epoch 056] mean loss = 3.166893 (batches=5)\n",
      "[epoch 057] mean loss = 3.044830 (batches=5)\n",
      "[epoch 058] mean loss = 3.069595 (batches=5)\n",
      "[epoch 059] mean loss = 2.998346 (batches=5)\n",
      "[epoch 060] mean loss = 3.102830 (batches=5)\n",
      "[epoch 061] mean loss = 3.085999 (batches=5)\n",
      "[epoch 062] mean loss = 2.988897 (batches=5)\n",
      "[epoch 063] mean loss = 2.970031 (batches=5)\n",
      "[epoch 064] mean loss = 2.985445 (batches=5)\n",
      "[epoch 065] mean loss = 2.945901 (batches=5)\n",
      "[epoch 066] mean loss = 2.961201 (batches=5)\n",
      "[epoch 067] mean loss = 2.965974 (batches=5)\n",
      "[epoch 068] mean loss = 2.905357 (batches=5)\n",
      "[epoch 069] mean loss = 2.872091 (batches=5)\n",
      "[epoch 070] mean loss = 2.879180 (batches=5)\n",
      "[epoch 071] mean loss = 2.926439 (batches=5)\n",
      "[epoch 072] mean loss = 2.960640 (batches=5)\n",
      "[epoch 073] mean loss = 2.903955 (batches=5)\n",
      "[epoch 074] mean loss = 2.794276 (batches=5)\n",
      "[epoch 075] mean loss = 2.854804 (batches=5)\n",
      "[epoch 076] mean loss = 2.786563 (batches=5)\n",
      "[epoch 077] mean loss = 2.692538 (batches=5)\n",
      "[epoch 078] mean loss = 2.750710 (batches=5)\n",
      "[epoch 079] mean loss = 2.780477 (batches=5)\n",
      "[epoch 080] mean loss = 2.764374 (batches=5)\n",
      "[epoch 081] mean loss = 2.805438 (batches=5)\n",
      "[epoch 082] mean loss = 2.718307 (batches=5)\n",
      "[epoch 083] mean loss = 2.733874 (batches=5)\n",
      "[epoch 084] mean loss = 2.751778 (batches=5)\n",
      "[epoch 085] mean loss = 2.619515 (batches=5)\n",
      "[epoch 086] mean loss = 2.702159 (batches=5)\n",
      "[epoch 087] mean loss = 2.701458 (batches=5)\n",
      "[epoch 088] mean loss = 2.629535 (batches=5)\n",
      "[epoch 089] mean loss = 2.604339 (batches=5)\n",
      "[epoch 090] mean loss = 2.641516 (batches=5)\n",
      "[epoch 091] mean loss = 2.714977 (batches=5)\n",
      "[epoch 092] mean loss = 2.642024 (batches=5)\n",
      "[epoch 093] mean loss = 2.623021 (batches=5)\n",
      "[epoch 094] mean loss = 2.625034 (batches=5)\n",
      "[epoch 095] mean loss = 2.601491 (batches=5)\n",
      "[epoch 096] mean loss = 2.567598 (batches=5)\n",
      "[epoch 097] mean loss = 2.620817 (batches=5)\n",
      "[epoch 098] mean loss = 2.623775 (batches=5)\n",
      "[epoch 099] mean loss = 2.583511 (batches=5)\n",
      "[epoch 100] mean loss = 2.602477 (batches=5)\n",
      "[epoch 101] mean loss = 2.573482 (batches=5)\n",
      "[epoch 102] mean loss = 2.591632 (batches=5)\n",
      "[epoch 103] mean loss = 2.600706 (batches=5)\n",
      "[epoch 104] mean loss = 2.588568 (batches=5)\n",
      "[epoch 105] mean loss = 2.555263 (batches=5)\n",
      "[epoch 106] mean loss = 2.656391 (batches=5)\n",
      "[epoch 107] mean loss = 2.517845 (batches=5)\n",
      "[epoch 108] mean loss = 2.511023 (batches=5)\n",
      "[epoch 109] mean loss = 2.621434 (batches=5)\n",
      "[epoch 110] mean loss = 2.492549 (batches=5)\n",
      "[epoch 111] mean loss = 2.433064 (batches=5)\n",
      "[epoch 112] mean loss = 2.554914 (batches=5)\n",
      "[epoch 113] mean loss = 2.457910 (batches=5)\n",
      "[epoch 114] mean loss = 2.432383 (batches=5)\n",
      "[epoch 115] mean loss = 2.473731 (batches=5)\n",
      "[epoch 116] mean loss = 2.414359 (batches=5)\n",
      "[epoch 117] mean loss = 2.414867 (batches=5)\n",
      "[epoch 118] mean loss = 2.339963 (batches=5)\n",
      "[epoch 119] mean loss = 2.404983 (batches=5)\n",
      "[epoch 120] mean loss = 2.369079 (batches=5)\n",
      "[epoch 121] mean loss = 2.495939 (batches=5)\n",
      "[epoch 122] mean loss = 2.399032 (batches=5)\n",
      "[epoch 123] mean loss = 2.373536 (batches=5)\n",
      "[epoch 124] mean loss = 2.363741 (batches=5)\n",
      "[epoch 125] mean loss = 2.350899 (batches=5)\n",
      "[epoch 126] mean loss = 2.417186 (batches=5)\n",
      "[epoch 127] mean loss = 2.358497 (batches=5)\n",
      "[epoch 128] mean loss = 2.408538 (batches=5)\n",
      "[epoch 129] mean loss = 2.370304 (batches=5)\n",
      "[epoch 130] mean loss = 2.321569 (batches=5)\n",
      "[epoch 131] mean loss = 2.286505 (batches=5)\n",
      "[epoch 132] mean loss = 2.312947 (batches=5)\n",
      "[epoch 133] mean loss = 2.273569 (batches=5)\n",
      "[epoch 134] mean loss = 2.329160 (batches=5)\n",
      "[epoch 135] mean loss = 2.345926 (batches=5)\n",
      "[epoch 136] mean loss = 2.299483 (batches=5)\n",
      "[epoch 137] mean loss = 2.318404 (batches=5)\n",
      "[epoch 138] mean loss = 2.339204 (batches=5)\n",
      "[epoch 139] mean loss = 2.303173 (batches=5)\n",
      "[epoch 140] mean loss = 2.292781 (batches=5)\n",
      "[epoch 141] mean loss = 2.343479 (batches=5)\n",
      "[epoch 142] mean loss = 2.223338 (batches=5)\n",
      "[epoch 143] mean loss = 2.326217 (batches=5)\n",
      "[epoch 144] mean loss = 2.284477 (batches=5)\n",
      "[epoch 145] mean loss = 2.310031 (batches=5)\n",
      "[epoch 146] mean loss = 2.332180 (batches=5)\n",
      "[epoch 147] mean loss = 2.237496 (batches=5)\n",
      "[epoch 148] mean loss = 2.221584 (batches=5)\n",
      "[epoch 149] mean loss = 2.329594 (batches=5)\n",
      "[epoch 150] mean loss = 2.353382 (batches=5)\n",
      "[epoch 151] mean loss = 2.314676 (batches=5)\n",
      "[epoch 152] mean loss = 2.236826 (batches=5)\n",
      "[epoch 153] mean loss = 2.296181 (batches=5)\n",
      "[epoch 154] mean loss = 2.381898 (batches=5)\n",
      "[epoch 155] mean loss = 2.220012 (batches=5)\n",
      "[epoch 156] mean loss = 2.286775 (batches=5)\n",
      "[epoch 157] mean loss = 2.200979 (batches=5)\n",
      "[epoch 158] mean loss = 2.151822 (batches=5)\n",
      "[epoch 159] mean loss = 2.198725 (batches=5)\n",
      "[epoch 160] mean loss = 2.258110 (batches=5)\n",
      "[epoch 161] mean loss = 2.168264 (batches=5)\n",
      "[epoch 162] mean loss = 2.191795 (batches=5)\n",
      "[epoch 163] mean loss = 2.219847 (batches=5)\n",
      "[epoch 164] mean loss = 2.246908 (batches=5)\n",
      "[epoch 165] mean loss = 2.185740 (batches=5)\n",
      "[epoch 166] mean loss = 2.099476 (batches=5)\n",
      "[epoch 167] mean loss = 2.086960 (batches=5)\n",
      "[epoch 168] mean loss = 2.197371 (batches=5)\n",
      "[epoch 169] mean loss = 2.141853 (batches=5)\n",
      "[epoch 170] mean loss = 2.141178 (batches=5)\n",
      "[epoch 171] mean loss = 2.284455 (batches=5)\n",
      "[epoch 172] mean loss = 2.123740 (batches=5)\n",
      "[epoch 173] mean loss = 2.182281 (batches=5)\n",
      "[epoch 174] mean loss = 2.136442 (batches=5)\n",
      "[epoch 175] mean loss = 2.205095 (batches=5)\n",
      "[epoch 176] mean loss = 2.140755 (batches=5)\n",
      "[epoch 177] mean loss = 2.219670 (batches=5)\n",
      "[epoch 178] mean loss = 2.088157 (batches=5)\n",
      "[epoch 179] mean loss = 2.259676 (batches=5)\n",
      "[epoch 180] mean loss = 2.140004 (batches=5)\n",
      "[epoch 181] mean loss = 2.252378 (batches=5)\n",
      "[epoch 182] mean loss = 2.183804 (batches=5)\n",
      "[epoch 183] mean loss = 2.059422 (batches=5)\n",
      "[epoch 184] mean loss = 2.094579 (batches=5)\n",
      "[epoch 185] mean loss = 2.186527 (batches=5)\n",
      "[epoch 186] mean loss = 2.066913 (batches=5)\n",
      "[epoch 187] mean loss = 2.110454 (batches=5)\n",
      "[epoch 188] mean loss = 2.052587 (batches=5)\n",
      "[epoch 189] mean loss = 2.133496 (batches=5)\n",
      "[epoch 190] mean loss = 2.099232 (batches=5)\n",
      "[epoch 191] mean loss = 2.018539 (batches=5)\n",
      "[epoch 192] mean loss = 2.059340 (batches=5)\n",
      "[epoch 193] mean loss = 1.984076 (batches=5)\n",
      "[epoch 194] mean loss = 1.994201 (batches=5)\n",
      "[epoch 195] mean loss = 2.094803 (batches=5)\n",
      "[epoch 196] mean loss = 2.052885 (batches=5)\n",
      "[epoch 197] mean loss = 1.962435 (batches=5)\n",
      "[epoch 198] mean loss = 2.050734 (batches=5)\n",
      "[epoch 199] mean loss = 2.015768 (batches=5)\n",
      "[epoch 200] mean loss = 2.042303 (batches=5)\n",
      "[epoch 201] mean loss = 2.030046 (batches=5)\n",
      "[epoch 202] mean loss = 2.063051 (batches=5)\n",
      "[epoch 203] mean loss = 1.988722 (batches=5)\n",
      "[epoch 204] mean loss = 2.072528 (batches=5)\n",
      "[epoch 205] mean loss = 2.041422 (batches=5)\n",
      "[epoch 206] mean loss = 1.995451 (batches=5)\n",
      "[epoch 207] mean loss = 2.020290 (batches=5)\n",
      "[epoch 208] mean loss = 2.102611 (batches=5)\n",
      "[epoch 209] mean loss = 2.027402 (batches=5)\n",
      "[epoch 210] mean loss = 2.044510 (batches=5)\n",
      "[epoch 211] mean loss = 1.979725 (batches=5)\n",
      "[epoch 212] mean loss = 2.046703 (batches=5)\n",
      "[epoch 213] mean loss = 2.005918 (batches=5)\n",
      "[epoch 214] mean loss = 1.954572 (batches=5)\n",
      "[epoch 215] mean loss = 1.966999 (batches=5)\n",
      "[epoch 216] mean loss = 2.009091 (batches=5)\n",
      "[epoch 217] mean loss = 1.935354 (batches=5)\n",
      "[epoch 218] mean loss = 1.965129 (batches=5)\n",
      "[epoch 219] mean loss = 1.962228 (batches=5)\n",
      "[epoch 220] mean loss = 1.964355 (batches=5)\n",
      "[epoch 221] mean loss = 1.957716 (batches=5)\n",
      "[epoch 222] mean loss = 1.993324 (batches=5)\n",
      "[epoch 223] mean loss = 1.935709 (batches=5)\n",
      "[epoch 224] mean loss = 1.963607 (batches=5)\n",
      "[epoch 225] mean loss = 1.955261 (batches=5)\n",
      "[epoch 226] mean loss = 1.922642 (batches=5)\n",
      "[epoch 227] mean loss = 1.917034 (batches=5)\n",
      "[epoch 228] mean loss = 1.952288 (batches=5)\n",
      "[epoch 229] mean loss = 1.964792 (batches=5)\n",
      "[epoch 230] mean loss = 1.933816 (batches=5)\n",
      "[epoch 231] mean loss = 1.914508 (batches=5)\n",
      "[epoch 232] mean loss = 1.935509 (batches=5)\n",
      "[epoch 233] mean loss = 1.856616 (batches=5)\n",
      "[epoch 234] mean loss = 2.031010 (batches=5)\n",
      "[epoch 235] mean loss = 1.984338 (batches=5)\n",
      "[epoch 236] mean loss = 1.881049 (batches=5)\n",
      "[epoch 237] mean loss = 1.979969 (batches=5)\n",
      "[epoch 238] mean loss = 1.996036 (batches=5)\n",
      "[epoch 239] mean loss = 1.885595 (batches=5)\n",
      "[epoch 240] mean loss = 1.904629 (batches=5)\n",
      "[epoch 241] mean loss = 1.812096 (batches=5)\n",
      "[epoch 242] mean loss = 1.874910 (batches=5)\n",
      "[epoch 243] mean loss = 1.774245 (batches=5)\n",
      "[epoch 244] mean loss = 1.852900 (batches=5)\n",
      "[epoch 245] mean loss = 1.828511 (batches=5)\n",
      "[epoch 246] mean loss = 1.838861 (batches=5)\n",
      "[epoch 247] mean loss = 1.776288 (batches=5)\n",
      "[epoch 248] mean loss = 1.909793 (batches=5)\n",
      "[epoch 249] mean loss = 2.002196 (batches=5)\n",
      "[epoch 250] mean loss = 1.952305 (batches=5)\n",
      "[epoch 251] mean loss = 1.835397 (batches=5)\n",
      "[epoch 252] mean loss = 1.774196 (batches=5)\n",
      "[epoch 253] mean loss = 1.822351 (batches=5)\n",
      "[epoch 254] mean loss = 1.918546 (batches=5)\n",
      "[epoch 255] mean loss = 1.795095 (batches=5)\n",
      "[epoch 256] mean loss = 1.882342 (batches=5)\n",
      "[epoch 257] mean loss = 1.922040 (batches=5)\n",
      "[epoch 258] mean loss = 1.963720 (batches=5)\n",
      "[epoch 259] mean loss = 1.909524 (batches=5)\n",
      "[epoch 260] mean loss = 1.892057 (batches=5)\n",
      "[epoch 261] mean loss = 1.886403 (batches=5)\n",
      "[epoch 262] mean loss = 1.851556 (batches=5)\n",
      "[epoch 263] mean loss = 1.741043 (batches=5)\n",
      "[epoch 264] mean loss = 1.786225 (batches=5)\n",
      "[epoch 265] mean loss = 1.746059 (batches=5)\n",
      "[epoch 266] mean loss = 1.791985 (batches=5)\n",
      "[epoch 267] mean loss = 1.778663 (batches=5)\n",
      "[epoch 268] mean loss = 1.772832 (batches=5)\n",
      "[epoch 269] mean loss = 1.851646 (batches=5)\n",
      "[epoch 270] mean loss = 1.815293 (batches=5)\n",
      "[epoch 271] mean loss = 1.755687 (batches=5)\n",
      "[epoch 272] mean loss = 1.746755 (batches=5)\n",
      "[epoch 273] mean loss = 1.846879 (batches=5)\n",
      "[epoch 274] mean loss = 1.778334 (batches=5)\n",
      "[epoch 275] mean loss = 1.834240 (batches=5)\n",
      "[epoch 276] mean loss = 1.790495 (batches=5)\n",
      "[epoch 277] mean loss = 1.776116 (batches=5)\n",
      "[epoch 278] mean loss = 1.846182 (batches=5)\n",
      "[epoch 279] mean loss = 1.766897 (batches=5)\n",
      "[epoch 280] mean loss = 1.763554 (batches=5)\n",
      "[epoch 281] mean loss = 1.777424 (batches=5)\n",
      "[epoch 282] mean loss = 1.823074 (batches=5)\n",
      "[epoch 283] mean loss = 1.720095 (batches=5)\n",
      "[epoch 284] mean loss = 1.763597 (batches=5)\n",
      "[epoch 285] mean loss = 1.800396 (batches=5)\n",
      "[epoch 286] mean loss = 1.899256 (batches=5)\n",
      "[epoch 287] mean loss = 1.733694 (batches=5)\n",
      "[epoch 288] mean loss = 1.746385 (batches=5)\n",
      "[epoch 289] mean loss = 1.703459 (batches=5)\n",
      "[epoch 290] mean loss = 1.804459 (batches=5)\n",
      "[epoch 291] mean loss = 1.688001 (batches=5)\n",
      "[epoch 292] mean loss = 1.764690 (batches=5)\n",
      "[epoch 293] mean loss = 1.708566 (batches=5)\n",
      "[epoch 294] mean loss = 1.717843 (batches=5)\n",
      "[epoch 295] mean loss = 1.740103 (batches=5)\n",
      "[epoch 296] mean loss = 1.667135 (batches=5)\n",
      "[epoch 297] mean loss = 1.631142 (batches=5)\n",
      "[epoch 298] mean loss = 1.734221 (batches=5)\n",
      "[epoch 299] mean loss = 1.702874 (batches=5)\n",
      "[epoch 300] mean loss = 1.663856 (batches=5)\n",
      "[epoch 301] mean loss = 1.731928 (batches=5)\n",
      "[epoch 302] mean loss = 1.786711 (batches=5)\n",
      "[epoch 303] mean loss = 1.681162 (batches=5)\n",
      "[epoch 304] mean loss = 1.786837 (batches=5)\n",
      "[epoch 305] mean loss = 1.702395 (batches=5)\n",
      "[epoch 306] mean loss = 1.761354 (batches=5)\n",
      "[epoch 307] mean loss = 1.673062 (batches=5)\n",
      "[epoch 308] mean loss = 1.747130 (batches=5)\n",
      "[epoch 309] mean loss = 1.791123 (batches=5)\n",
      "[epoch 310] mean loss = 1.766993 (batches=5)\n",
      "[epoch 311] mean loss = 1.668461 (batches=5)\n",
      "[epoch 312] mean loss = 1.656469 (batches=5)\n",
      "[epoch 313] mean loss = 1.770679 (batches=5)\n",
      "[epoch 314] mean loss = 1.728368 (batches=5)\n",
      "[epoch 315] mean loss = 1.645317 (batches=5)\n",
      "[epoch 316] mean loss = 1.644080 (batches=5)\n",
      "[epoch 317] mean loss = 1.701414 (batches=5)\n",
      "[epoch 318] mean loss = 1.631343 (batches=5)\n",
      "[epoch 319] mean loss = 1.665815 (batches=5)\n",
      "[epoch 320] mean loss = 1.648791 (batches=5)\n",
      "[epoch 321] mean loss = 1.687769 (batches=5)\n",
      "[epoch 322] mean loss = 1.622726 (batches=5)\n",
      "[epoch 323] mean loss = 1.686281 (batches=5)\n",
      "[epoch 324] mean loss = 1.619709 (batches=5)\n",
      "[epoch 325] mean loss = 1.591545 (batches=5)\n",
      "[epoch 326] mean loss = 1.555160 (batches=5)\n",
      "[epoch 327] mean loss = 1.586419 (batches=5)\n",
      "[epoch 328] mean loss = 1.647508 (batches=5)\n",
      "[epoch 329] mean loss = 1.634811 (batches=5)\n",
      "[epoch 330] mean loss = 1.592640 (batches=5)\n",
      "[epoch 331] mean loss = 1.672231 (batches=5)\n",
      "[epoch 332] mean loss = 1.577391 (batches=5)\n",
      "[epoch 333] mean loss = 1.627480 (batches=5)\n",
      "[epoch 334] mean loss = 1.632526 (batches=5)\n",
      "[epoch 335] mean loss = 1.608060 (batches=5)\n",
      "[epoch 336] mean loss = 1.651821 (batches=5)\n",
      "[epoch 337] mean loss = 1.620246 (batches=5)\n",
      "[epoch 338] mean loss = 1.547641 (batches=5)\n",
      "[epoch 339] mean loss = 1.580270 (batches=5)\n",
      "[epoch 340] mean loss = 1.678706 (batches=5)\n",
      "[epoch 341] mean loss = 1.586612 (batches=5)\n",
      "[epoch 342] mean loss = 1.606169 (batches=5)\n",
      "[epoch 343] mean loss = 1.534458 (batches=5)\n",
      "[epoch 344] mean loss = 1.621607 (batches=5)\n",
      "[epoch 345] mean loss = 1.490467 (batches=5)\n",
      "[epoch 346] mean loss = 1.568496 (batches=5)\n",
      "[epoch 347] mean loss = 1.537694 (batches=5)\n",
      "[epoch 348] mean loss = 1.537556 (batches=5)\n",
      "[epoch 349] mean loss = 1.557401 (batches=5)\n",
      "[epoch 350] mean loss = 1.544965 (batches=5)\n",
      "[epoch 351] mean loss = 1.569528 (batches=5)\n",
      "[epoch 352] mean loss = 1.567253 (batches=5)\n",
      "[epoch 353] mean loss = 1.564477 (batches=5)\n",
      "[epoch 354] mean loss = 1.581272 (batches=5)\n",
      "[epoch 355] mean loss = 1.424451 (batches=5)\n",
      "[epoch 356] mean loss = 1.540445 (batches=5)\n",
      "[epoch 357] mean loss = 1.583822 (batches=5)\n",
      "[epoch 358] mean loss = 1.499358 (batches=5)\n",
      "[epoch 359] mean loss = 1.545910 (batches=5)\n",
      "[epoch 360] mean loss = 1.530505 (batches=5)\n",
      "[epoch 361] mean loss = 1.565469 (batches=5)\n",
      "[epoch 362] mean loss = 1.594365 (batches=5)\n",
      "[epoch 363] mean loss = 1.535266 (batches=5)\n",
      "[epoch 364] mean loss = 1.528720 (batches=5)\n",
      "[epoch 365] mean loss = 1.583672 (batches=5)\n",
      "[epoch 366] mean loss = 1.660894 (batches=5)\n",
      "[epoch 367] mean loss = 1.593252 (batches=5)\n",
      "[epoch 368] mean loss = 1.544677 (batches=5)\n",
      "[epoch 369] mean loss = 1.530495 (batches=5)\n",
      "[epoch 370] mean loss = 1.536708 (batches=5)\n",
      "[epoch 371] mean loss = 1.520120 (batches=5)\n",
      "[epoch 372] mean loss = 1.545453 (batches=5)\n",
      "[epoch 373] mean loss = 1.560156 (batches=5)\n",
      "[epoch 374] mean loss = 1.594656 (batches=5)\n",
      "[epoch 375] mean loss = 1.568190 (batches=5)\n",
      "[epoch 376] mean loss = 1.437188 (batches=5)\n",
      "[epoch 377] mean loss = 1.578679 (batches=5)\n",
      "[epoch 378] mean loss = 1.487814 (batches=5)\n",
      "[epoch 379] mean loss = 1.502805 (batches=5)\n",
      "[epoch 380] mean loss = 1.492323 (batches=5)\n",
      "[epoch 381] mean loss = 1.519146 (batches=5)\n",
      "[epoch 382] mean loss = 1.416467 (batches=5)\n",
      "[epoch 383] mean loss = 1.479939 (batches=5)\n",
      "[epoch 384] mean loss = 1.512390 (batches=5)\n",
      "[epoch 385] mean loss = 1.547774 (batches=5)\n",
      "[epoch 386] mean loss = 1.566048 (batches=5)\n",
      "[epoch 387] mean loss = 1.518937 (batches=5)\n",
      "[epoch 388] mean loss = 1.524823 (batches=5)\n",
      "[epoch 389] mean loss = 1.552349 (batches=5)\n",
      "[epoch 390] mean loss = 1.461877 (batches=5)\n",
      "[epoch 391] mean loss = 1.460524 (batches=5)\n",
      "[epoch 392] mean loss = 1.532676 (batches=5)\n",
      "[epoch 393] mean loss = 1.544157 (batches=5)\n",
      "[epoch 394] mean loss = 1.560911 (batches=5)\n",
      "[epoch 395] mean loss = 1.542569 (batches=5)\n",
      "[epoch 396] mean loss = 1.502891 (batches=5)\n",
      "[epoch 397] mean loss = 1.491478 (batches=5)\n",
      "[epoch 398] mean loss = 1.556411 (batches=5)\n",
      "[epoch 399] mean loss = 1.547086 (batches=5)\n",
      "[epoch 400] mean loss = 1.524312 (batches=5)\n",
      "[epoch 401] mean loss = 1.578231 (batches=5)\n",
      "[epoch 402] mean loss = 1.520169 (batches=5)\n",
      "[epoch 403] mean loss = 1.489762 (batches=5)\n",
      "[epoch 404] mean loss = 1.481704 (batches=5)\n",
      "[epoch 405] mean loss = 1.442498 (batches=5)\n",
      "[epoch 406] mean loss = 1.536213 (batches=5)\n",
      "[epoch 407] mean loss = 1.572186 (batches=5)\n",
      "[epoch 408] mean loss = 1.492015 (batches=5)\n",
      "[epoch 409] mean loss = 1.515913 (batches=5)\n",
      "[epoch 410] mean loss = 1.488680 (batches=5)\n",
      "[epoch 411] mean loss = 1.473198 (batches=5)\n",
      "[epoch 412] mean loss = 1.450911 (batches=5)\n",
      "[epoch 413] mean loss = 1.454585 (batches=5)\n",
      "[epoch 414] mean loss = 1.381273 (batches=5)\n",
      "[epoch 415] mean loss = 1.416840 (batches=5)\n",
      "[epoch 416] mean loss = 1.516971 (batches=5)\n",
      "[epoch 417] mean loss = 1.441570 (batches=5)\n",
      "[epoch 418] mean loss = 1.521456 (batches=5)\n",
      "[epoch 419] mean loss = 1.481989 (batches=5)\n",
      "[epoch 420] mean loss = 1.436075 (batches=5)\n",
      "[epoch 421] mean loss = 1.390687 (batches=5)\n",
      "[epoch 422] mean loss = 1.323791 (batches=5)\n",
      "[epoch 423] mean loss = 1.448455 (batches=5)\n",
      "[epoch 424] mean loss = 1.424536 (batches=5)\n",
      "[epoch 425] mean loss = 1.421093 (batches=5)\n",
      "[epoch 426] mean loss = 1.402312 (batches=5)\n",
      "[epoch 427] mean loss = 1.504038 (batches=5)\n",
      "[epoch 428] mean loss = 1.441220 (batches=5)\n",
      "[epoch 429] mean loss = 1.400427 (batches=5)\n",
      "[epoch 430] mean loss = 1.479475 (batches=5)\n",
      "[epoch 431] mean loss = 1.446531 (batches=5)\n",
      "[epoch 432] mean loss = 1.400246 (batches=5)\n",
      "[epoch 433] mean loss = 1.386449 (batches=5)\n",
      "[epoch 434] mean loss = 1.390737 (batches=5)\n",
      "[epoch 435] mean loss = 1.312070 (batches=5)\n",
      "[epoch 436] mean loss = 1.409478 (batches=5)\n",
      "[epoch 437] mean loss = 1.460087 (batches=5)\n",
      "[epoch 438] mean loss = 1.403043 (batches=5)\n",
      "[epoch 439] mean loss = 1.390599 (batches=5)\n",
      "[epoch 440] mean loss = 1.385269 (batches=5)\n",
      "[epoch 441] mean loss = 1.388819 (batches=5)\n",
      "[epoch 442] mean loss = 1.341549 (batches=5)\n",
      "[epoch 443] mean loss = 1.421671 (batches=5)\n",
      "[epoch 444] mean loss = 1.341642 (batches=5)\n",
      "[epoch 445] mean loss = 1.366500 (batches=5)\n",
      "[epoch 446] mean loss = 1.495429 (batches=5)\n",
      "[epoch 447] mean loss = 1.479749 (batches=5)\n",
      "[epoch 448] mean loss = 1.431643 (batches=5)\n",
      "[epoch 449] mean loss = 1.335186 (batches=5)\n",
      "[epoch 450] mean loss = 1.363270 (batches=5)\n",
      "[epoch 451] mean loss = 1.350092 (batches=5)\n",
      "[epoch 452] mean loss = 1.332594 (batches=5)\n",
      "[epoch 453] mean loss = 1.326394 (batches=5)\n",
      "[epoch 454] mean loss = 1.315298 (batches=5)\n",
      "[epoch 455] mean loss = 1.312705 (batches=5)\n",
      "[epoch 456] mean loss = 1.344662 (batches=5)\n",
      "[epoch 457] mean loss = 1.324555 (batches=5)\n",
      "[epoch 458] mean loss = 1.340048 (batches=5)\n",
      "[epoch 459] mean loss = 1.341416 (batches=5)\n",
      "[epoch 460] mean loss = 1.317947 (batches=5)\n",
      "[epoch 461] mean loss = 1.350995 (batches=5)\n",
      "[epoch 462] mean loss = 1.295246 (batches=5)\n",
      "[epoch 463] mean loss = 1.385381 (batches=5)\n",
      "[epoch 464] mean loss = 1.343723 (batches=5)\n",
      "[epoch 465] mean loss = 1.273495 (batches=5)\n",
      "[epoch 466] mean loss = 1.317616 (batches=5)\n",
      "[epoch 467] mean loss = 1.356710 (batches=5)\n",
      "[epoch 468] mean loss = 1.362302 (batches=5)\n",
      "[epoch 469] mean loss = 1.331430 (batches=5)\n",
      "[epoch 470] mean loss = 1.476400 (batches=5)\n",
      "[epoch 471] mean loss = 1.352340 (batches=5)\n",
      "[epoch 472] mean loss = 1.262938 (batches=5)\n",
      "[epoch 473] mean loss = 1.359139 (batches=5)\n",
      "[epoch 474] mean loss = 1.362709 (batches=5)\n",
      "[epoch 475] mean loss = 1.327862 (batches=5)\n",
      "[epoch 476] mean loss = 1.315627 (batches=5)\n",
      "[epoch 477] mean loss = 1.314721 (batches=5)\n",
      "[epoch 478] mean loss = 1.288979 (batches=5)\n",
      "[epoch 479] mean loss = 1.295281 (batches=5)\n",
      "[epoch 480] mean loss = 1.263243 (batches=5)\n",
      "[epoch 481] mean loss = 1.269542 (batches=5)\n",
      "[epoch 482] mean loss = 1.260762 (batches=5)\n",
      "[epoch 483] mean loss = 1.338397 (batches=5)\n",
      "[epoch 484] mean loss = 1.322208 (batches=5)\n",
      "[epoch 485] mean loss = 1.308521 (batches=5)\n",
      "[epoch 486] mean loss = 1.264230 (batches=5)\n",
      "[epoch 487] mean loss = 1.297292 (batches=5)\n",
      "[epoch 488] mean loss = 1.293679 (batches=5)\n",
      "[epoch 489] mean loss = 1.222471 (batches=5)\n",
      "[epoch 490] mean loss = 1.244149 (batches=5)\n",
      "[epoch 491] mean loss = 1.402710 (batches=5)\n",
      "[epoch 492] mean loss = 1.286972 (batches=5)\n",
      "[epoch 493] mean loss = 1.274368 (batches=5)\n",
      "[epoch 494] mean loss = 1.327863 (batches=5)\n",
      "[epoch 495] mean loss = 1.262839 (batches=5)\n",
      "[epoch 496] mean loss = 1.327836 (batches=5)\n",
      "[epoch 497] mean loss = 1.316053 (batches=5)\n",
      "[epoch 498] mean loss = 1.267682 (batches=5)\n",
      "[epoch 499] mean loss = 1.287226 (batches=5)\n",
      "[epoch 500] mean loss = 1.300487 (batches=5)\n",
      "[epoch 501] mean loss = 1.269288 (batches=5)\n",
      "[epoch 502] mean loss = 1.215628 (batches=5)\n",
      "[epoch 503] mean loss = 1.300208 (batches=5)\n",
      "[epoch 504] mean loss = 1.317376 (batches=5)\n",
      "[epoch 505] mean loss = 1.253419 (batches=5)\n",
      "[epoch 506] mean loss = 1.276192 (batches=5)\n",
      "[epoch 507] mean loss = 1.280326 (batches=5)\n",
      "[epoch 508] mean loss = 1.248818 (batches=5)\n",
      "[epoch 509] mean loss = 1.173508 (batches=5)\n",
      "[epoch 510] mean loss = 1.238173 (batches=5)\n",
      "[epoch 511] mean loss = 1.308567 (batches=5)\n",
      "[epoch 512] mean loss = 1.273730 (batches=5)\n",
      "[epoch 513] mean loss = 1.218759 (batches=5)\n",
      "[epoch 514] mean loss = 1.272905 (batches=5)\n",
      "[epoch 515] mean loss = 1.284300 (batches=5)\n",
      "[epoch 516] mean loss = 1.275659 (batches=5)\n",
      "[epoch 517] mean loss = 1.351737 (batches=5)\n",
      "[epoch 518] mean loss = 1.277665 (batches=5)\n",
      "[epoch 519] mean loss = 1.248073 (batches=5)\n",
      "[epoch 520] mean loss = 1.327141 (batches=5)\n",
      "[epoch 521] mean loss = 1.252615 (batches=5)\n",
      "[epoch 522] mean loss = 1.351330 (batches=5)\n",
      "[epoch 523] mean loss = 1.183457 (batches=5)\n",
      "[epoch 524] mean loss = 1.237216 (batches=5)\n",
      "[epoch 525] mean loss = 1.260016 (batches=5)\n",
      "[epoch 526] mean loss = 1.219837 (batches=5)\n",
      "[epoch 527] mean loss = 1.204352 (batches=5)\n",
      "[epoch 528] mean loss = 1.245821 (batches=5)\n",
      "[epoch 529] mean loss = 1.214001 (batches=5)\n",
      "[epoch 530] mean loss = 1.278577 (batches=5)\n",
      "[epoch 531] mean loss = 1.176243 (batches=5)\n",
      "[epoch 532] mean loss = 1.263977 (batches=5)\n",
      "[epoch 533] mean loss = 1.275960 (batches=5)\n",
      "[epoch 534] mean loss = 1.249677 (batches=5)\n",
      "[epoch 535] mean loss = 1.297779 (batches=5)\n",
      "[epoch 536] mean loss = 1.271145 (batches=5)\n",
      "[epoch 537] mean loss = 1.341458 (batches=5)\n",
      "[epoch 538] mean loss = 1.245333 (batches=5)\n",
      "[epoch 539] mean loss = 1.269276 (batches=5)\n",
      "[epoch 540] mean loss = 1.259288 (batches=5)\n",
      "[epoch 541] mean loss = 1.259666 (batches=5)\n",
      "[epoch 542] mean loss = 1.180675 (batches=5)\n",
      "[epoch 543] mean loss = 1.153719 (batches=5)\n",
      "[epoch 544] mean loss = 1.210000 (batches=5)\n",
      "[epoch 545] mean loss = 1.262936 (batches=5)\n",
      "[epoch 546] mean loss = 1.258510 (batches=5)\n",
      "[epoch 547] mean loss = 1.191162 (batches=5)\n",
      "[epoch 548] mean loss = 1.236219 (batches=5)\n",
      "[epoch 549] mean loss = 1.222072 (batches=5)\n",
      "[epoch 550] mean loss = 1.203466 (batches=5)\n",
      "[epoch 551] mean loss = 1.205031 (batches=5)\n",
      "[epoch 552] mean loss = 1.215125 (batches=5)\n",
      "[epoch 553] mean loss = 1.228897 (batches=5)\n",
      "[epoch 554] mean loss = 1.271222 (batches=5)\n",
      "[epoch 555] mean loss = 1.219637 (batches=5)\n",
      "[epoch 556] mean loss = 1.174141 (batches=5)\n",
      "[epoch 557] mean loss = 1.125158 (batches=5)\n",
      "[epoch 558] mean loss = 1.141819 (batches=5)\n",
      "[epoch 559] mean loss = 1.208796 (batches=5)\n",
      "[epoch 560] mean loss = 1.234195 (batches=5)\n",
      "[epoch 561] mean loss = 1.149923 (batches=5)\n",
      "[epoch 562] mean loss = 1.259396 (batches=5)\n",
      "[epoch 563] mean loss = 1.162207 (batches=5)\n",
      "[epoch 564] mean loss = 1.197933 (batches=5)\n",
      "[epoch 565] mean loss = 1.162250 (batches=5)\n",
      "[epoch 566] mean loss = 1.219998 (batches=5)\n",
      "[epoch 567] mean loss = 1.218798 (batches=5)\n",
      "[epoch 568] mean loss = 1.205940 (batches=5)\n",
      "[epoch 569] mean loss = 1.165336 (batches=5)\n",
      "[epoch 570] mean loss = 1.138291 (batches=5)\n",
      "[epoch 571] mean loss = 1.184168 (batches=5)\n",
      "[epoch 572] mean loss = 1.108287 (batches=5)\n",
      "[epoch 573] mean loss = 1.177789 (batches=5)\n",
      "[epoch 574] mean loss = 1.205013 (batches=5)\n",
      "[epoch 575] mean loss = 1.159649 (batches=5)\n",
      "[epoch 576] mean loss = 1.155068 (batches=5)\n",
      "[epoch 577] mean loss = 1.167200 (batches=5)\n",
      "[epoch 578] mean loss = 1.164370 (batches=5)\n",
      "[epoch 579] mean loss = 1.186661 (batches=5)\n",
      "[epoch 580] mean loss = 1.161755 (batches=5)\n",
      "[epoch 581] mean loss = 1.226067 (batches=5)\n",
      "[epoch 582] mean loss = 1.160075 (batches=5)\n",
      "[epoch 583] mean loss = 1.204840 (batches=5)\n",
      "[epoch 584] mean loss = 1.146532 (batches=5)\n",
      "[epoch 585] mean loss = 1.205048 (batches=5)\n",
      "[epoch 586] mean loss = 1.118124 (batches=5)\n",
      "[epoch 587] mean loss = 0.999290 (batches=5)\n",
      "[epoch 588] mean loss = 1.142087 (batches=5)\n",
      "[epoch 589] mean loss = 1.204899 (batches=5)\n",
      "[epoch 590] mean loss = 1.215975 (batches=5)\n",
      "[epoch 591] mean loss = 1.127860 (batches=5)\n",
      "[epoch 592] mean loss = 1.133248 (batches=5)\n",
      "[epoch 593] mean loss = 1.123134 (batches=5)\n",
      "[epoch 594] mean loss = 1.176099 (batches=5)\n",
      "[epoch 595] mean loss = 1.158955 (batches=5)\n",
      "[epoch 596] mean loss = 1.088793 (batches=5)\n",
      "[epoch 597] mean loss = 1.138047 (batches=5)\n",
      "[epoch 598] mean loss = 1.101024 (batches=5)\n",
      "[epoch 599] mean loss = 1.183707 (batches=5)\n",
      "[epoch 600] mean loss = 1.179920 (batches=5)\n",
      "[epoch 601] mean loss = 1.116823 (batches=5)\n",
      "[epoch 602] mean loss = 1.095046 (batches=5)\n",
      "[epoch 603] mean loss = 1.170154 (batches=5)\n",
      "[epoch 604] mean loss = 1.140460 (batches=5)\n",
      "[epoch 605] mean loss = 1.095011 (batches=5)\n",
      "[epoch 606] mean loss = 1.151542 (batches=5)\n",
      "[epoch 607] mean loss = 1.097298 (batches=5)\n",
      "[epoch 608] mean loss = 1.121916 (batches=5)\n",
      "[epoch 609] mean loss = 1.137516 (batches=5)\n",
      "[epoch 610] mean loss = 1.073562 (batches=5)\n",
      "[epoch 611] mean loss = 1.099730 (batches=5)\n",
      "[epoch 612] mean loss = 1.099897 (batches=5)\n",
      "[epoch 613] mean loss = 1.069963 (batches=5)\n",
      "[epoch 614] mean loss = 1.105862 (batches=5)\n",
      "[epoch 615] mean loss = 1.035850 (batches=5)\n",
      "[epoch 616] mean loss = 1.131804 (batches=5)\n",
      "[epoch 617] mean loss = 1.068622 (batches=5)\n",
      "[epoch 618] mean loss = 1.094931 (batches=5)\n",
      "[epoch 619] mean loss = 1.107900 (batches=5)\n",
      "[epoch 620] mean loss = 1.111198 (batches=5)\n",
      "[epoch 621] mean loss = 1.067376 (batches=5)\n",
      "[epoch 622] mean loss = 1.146805 (batches=5)\n",
      "[epoch 623] mean loss = 1.084188 (batches=5)\n",
      "[epoch 624] mean loss = 1.056387 (batches=5)\n",
      "[epoch 625] mean loss = 1.028607 (batches=5)\n",
      "[epoch 626] mean loss = 1.033599 (batches=5)\n",
      "[epoch 627] mean loss = 1.063710 (batches=5)\n",
      "[epoch 628] mean loss = 1.076758 (batches=5)\n",
      "[epoch 629] mean loss = 1.066745 (batches=5)\n",
      "[epoch 630] mean loss = 1.098680 (batches=5)\n",
      "[epoch 631] mean loss = 1.081384 (batches=5)\n",
      "[epoch 632] mean loss = 1.052362 (batches=5)\n",
      "[epoch 633] mean loss = 1.155345 (batches=5)\n",
      "[epoch 634] mean loss = 1.179273 (batches=5)\n",
      "[epoch 635] mean loss = 1.148874 (batches=5)\n",
      "[epoch 636] mean loss = 1.135675 (batches=5)\n",
      "[epoch 637] mean loss = 1.189781 (batches=5)\n",
      "[epoch 638] mean loss = 1.135022 (batches=5)\n",
      "[epoch 639] mean loss = 1.094036 (batches=5)\n",
      "[epoch 640] mean loss = 1.081038 (batches=5)\n",
      "[epoch 641] mean loss = 1.143095 (batches=5)\n",
      "[epoch 642] mean loss = 1.028776 (batches=5)\n",
      "[epoch 643] mean loss = 1.165645 (batches=5)\n",
      "[epoch 644] mean loss = 1.027884 (batches=5)\n",
      "[epoch 645] mean loss = 1.191670 (batches=5)\n",
      "[epoch 646] mean loss = 1.061989 (batches=5)\n",
      "[epoch 647] mean loss = 1.117475 (batches=5)\n",
      "[epoch 648] mean loss = 1.093864 (batches=5)\n",
      "[epoch 649] mean loss = 1.133431 (batches=5)\n",
      "[epoch 650] mean loss = 1.109376 (batches=5)\n",
      "[epoch 651] mean loss = 1.055463 (batches=5)\n",
      "[epoch 652] mean loss = 1.066544 (batches=5)\n",
      "[epoch 653] mean loss = 1.138822 (batches=5)\n",
      "[epoch 654] mean loss = 1.122736 (batches=5)\n",
      "[epoch 655] mean loss = 1.089415 (batches=5)\n",
      "[epoch 656] mean loss = 1.121353 (batches=5)\n",
      "[epoch 657] mean loss = 1.025383 (batches=5)\n",
      "[epoch 658] mean loss = 1.116754 (batches=5)\n",
      "[epoch 659] mean loss = 1.015697 (batches=5)\n",
      "[epoch 660] mean loss = 1.080606 (batches=5)\n",
      "[epoch 661] mean loss = 1.077506 (batches=5)\n",
      "[epoch 662] mean loss = 1.124822 (batches=5)\n",
      "[epoch 663] mean loss = 1.134593 (batches=5)\n",
      "[epoch 664] mean loss = 1.111415 (batches=5)\n",
      "[epoch 665] mean loss = 1.114036 (batches=5)\n",
      "[epoch 666] mean loss = 1.099927 (batches=5)\n",
      "[epoch 667] mean loss = 1.068391 (batches=5)\n",
      "[epoch 668] mean loss = 1.058860 (batches=5)\n",
      "[epoch 669] mean loss = 1.008576 (batches=5)\n",
      "[epoch 670] mean loss = 1.007992 (batches=5)\n",
      "[epoch 671] mean loss = 1.029872 (batches=5)\n",
      "[epoch 672] mean loss = 1.004910 (batches=5)\n",
      "[epoch 673] mean loss = 0.987515 (batches=5)\n",
      "[epoch 674] mean loss = 1.068195 (batches=5)\n",
      "[epoch 675] mean loss = 1.059354 (batches=5)\n",
      "[epoch 676] mean loss = 1.064804 (batches=5)\n",
      "[epoch 677] mean loss = 1.074080 (batches=5)\n",
      "[epoch 678] mean loss = 0.984938 (batches=5)\n",
      "[epoch 679] mean loss = 0.998210 (batches=5)\n",
      "[epoch 680] mean loss = 1.004698 (batches=5)\n",
      "[epoch 681] mean loss = 0.981707 (batches=5)\n",
      "[epoch 682] mean loss = 0.965128 (batches=5)\n",
      "[epoch 683] mean loss = 1.092998 (batches=5)\n",
      "[epoch 684] mean loss = 1.058055 (batches=5)\n",
      "[epoch 685] mean loss = 1.056838 (batches=5)\n",
      "[epoch 686] mean loss = 1.094930 (batches=5)\n",
      "[epoch 687] mean loss = 1.112172 (batches=5)\n",
      "[epoch 688] mean loss = 1.138902 (batches=5)\n",
      "[epoch 689] mean loss = 1.110778 (batches=5)\n",
      "[epoch 690] mean loss = 1.029390 (batches=5)\n",
      "[epoch 691] mean loss = 1.009826 (batches=5)\n",
      "[epoch 692] mean loss = 1.006528 (batches=5)\n",
      "[epoch 693] mean loss = 1.034166 (batches=5)\n",
      "[epoch 694] mean loss = 1.026420 (batches=5)\n",
      "[epoch 695] mean loss = 0.941925 (batches=5)\n",
      "[epoch 696] mean loss = 1.102880 (batches=5)\n",
      "[epoch 697] mean loss = 1.035743 (batches=5)\n",
      "[epoch 698] mean loss = 1.042707 (batches=5)\n",
      "[epoch 699] mean loss = 1.047486 (batches=5)\n",
      "[epoch 700] mean loss = 1.029221 (batches=5)\n",
      "[epoch 701] mean loss = 1.064363 (batches=5)\n",
      "[epoch 702] mean loss = 1.022611 (batches=5)\n",
      "[epoch 703] mean loss = 1.024616 (batches=5)\n",
      "[epoch 704] mean loss = 1.012679 (batches=5)\n",
      "[epoch 705] mean loss = 1.166568 (batches=5)\n",
      "[epoch 706] mean loss = 0.988148 (batches=5)\n",
      "[epoch 707] mean loss = 1.048233 (batches=5)\n",
      "[epoch 708] mean loss = 1.077286 (batches=5)\n",
      "[epoch 709] mean loss = 0.990855 (batches=5)\n",
      "[epoch 710] mean loss = 0.996911 (batches=5)\n",
      "[epoch 711] mean loss = 0.968773 (batches=5)\n",
      "[epoch 712] mean loss = 0.959862 (batches=5)\n",
      "[epoch 713] mean loss = 0.953277 (batches=5)\n",
      "[epoch 714] mean loss = 0.973697 (batches=5)\n",
      "[epoch 715] mean loss = 1.012944 (batches=5)\n",
      "[epoch 716] mean loss = 1.008871 (batches=5)\n",
      "[epoch 717] mean loss = 0.982299 (batches=5)\n",
      "[epoch 718] mean loss = 1.027952 (batches=5)\n",
      "[epoch 719] mean loss = 1.013842 (batches=5)\n",
      "[epoch 720] mean loss = 1.019351 (batches=5)\n",
      "[epoch 721] mean loss = 1.038088 (batches=5)\n",
      "[epoch 722] mean loss = 0.976173 (batches=5)\n",
      "[epoch 723] mean loss = 0.979543 (batches=5)\n",
      "[epoch 724] mean loss = 0.966543 (batches=5)\n",
      "[epoch 725] mean loss = 0.892500 (batches=5)\n",
      "[epoch 726] mean loss = 0.961018 (batches=5)\n",
      "[epoch 727] mean loss = 1.098515 (batches=5)\n",
      "[epoch 728] mean loss = 0.972697 (batches=5)\n",
      "[epoch 729] mean loss = 0.979291 (batches=5)\n",
      "[epoch 730] mean loss = 0.977432 (batches=5)\n",
      "[epoch 731] mean loss = 0.928397 (batches=5)\n",
      "[epoch 732] mean loss = 0.964324 (batches=5)\n",
      "[epoch 733] mean loss = 1.035448 (batches=5)\n",
      "[epoch 734] mean loss = 1.050020 (batches=5)\n",
      "[epoch 735] mean loss = 0.981645 (batches=5)\n",
      "[epoch 736] mean loss = 0.974371 (batches=5)\n",
      "[epoch 737] mean loss = 0.999620 (batches=5)\n",
      "[epoch 738] mean loss = 1.021133 (batches=5)\n",
      "[epoch 739] mean loss = 0.997912 (batches=5)\n",
      "[epoch 740] mean loss = 0.900939 (batches=5)\n",
      "[epoch 741] mean loss = 0.941340 (batches=5)\n",
      "[epoch 742] mean loss = 0.942781 (batches=5)\n",
      "[epoch 743] mean loss = 0.971510 (batches=5)\n",
      "[epoch 744] mean loss = 1.011322 (batches=5)\n",
      "[epoch 745] mean loss = 0.955215 (batches=5)\n",
      "[epoch 746] mean loss = 0.950796 (batches=5)\n",
      "[epoch 747] mean loss = 0.902620 (batches=5)\n",
      "[epoch 748] mean loss = 0.911323 (batches=5)\n",
      "[epoch 749] mean loss = 0.923885 (batches=5)\n",
      "[epoch 750] mean loss = 0.970273 (batches=5)\n",
      "[epoch 751] mean loss = 0.932260 (batches=5)\n",
      "[epoch 752] mean loss = 0.945075 (batches=5)\n",
      "[epoch 753] mean loss = 0.910699 (batches=5)\n",
      "[epoch 754] mean loss = 0.972783 (batches=5)\n",
      "[epoch 755] mean loss = 0.988125 (batches=5)\n",
      "[epoch 756] mean loss = 0.959647 (batches=5)\n",
      "[epoch 757] mean loss = 0.932270 (batches=5)\n",
      "[epoch 758] mean loss = 0.904273 (batches=5)\n",
      "[epoch 759] mean loss = 0.943908 (batches=5)\n",
      "[epoch 760] mean loss = 0.970948 (batches=5)\n",
      "[epoch 761] mean loss = 0.969876 (batches=5)\n",
      "[epoch 762] mean loss = 0.921991 (batches=5)\n",
      "[epoch 763] mean loss = 0.903814 (batches=5)\n",
      "[epoch 764] mean loss = 0.938885 (batches=5)\n",
      "[epoch 765] mean loss = 0.883561 (batches=5)\n",
      "[epoch 766] mean loss = 0.878819 (batches=5)\n",
      "[epoch 767] mean loss = 0.886741 (batches=5)\n",
      "[epoch 768] mean loss = 0.943109 (batches=5)\n",
      "[epoch 769] mean loss = 0.951493 (batches=5)\n",
      "[epoch 770] mean loss = 0.935622 (batches=5)\n",
      "[epoch 771] mean loss = 0.937488 (batches=5)\n",
      "[epoch 772] mean loss = 0.917124 (batches=5)\n",
      "[epoch 773] mean loss = 0.938440 (batches=5)\n",
      "[epoch 774] mean loss = 0.937948 (batches=5)\n",
      "[epoch 775] mean loss = 0.904751 (batches=5)\n",
      "[epoch 776] mean loss = 0.952692 (batches=5)\n",
      "[epoch 777] mean loss = 0.992179 (batches=5)\n",
      "[epoch 778] mean loss = 0.946126 (batches=5)\n",
      "[epoch 779] mean loss = 0.972838 (batches=5)\n",
      "[epoch 780] mean loss = 0.936397 (batches=5)\n",
      "[epoch 781] mean loss = 0.984348 (batches=5)\n",
      "[epoch 782] mean loss = 0.942074 (batches=5)\n",
      "[epoch 783] mean loss = 0.951332 (batches=5)\n",
      "[epoch 784] mean loss = 0.944770 (batches=5)\n",
      "[epoch 785] mean loss = 0.886315 (batches=5)\n",
      "[epoch 786] mean loss = 0.958858 (batches=5)\n",
      "[epoch 787] mean loss = 0.870227 (batches=5)\n",
      "[epoch 788] mean loss = 0.954248 (batches=5)\n",
      "[epoch 789] mean loss = 0.894695 (batches=5)\n",
      "[epoch 790] mean loss = 0.931082 (batches=5)\n",
      "[epoch 791] mean loss = 0.917122 (batches=5)\n",
      "[epoch 792] mean loss = 0.882254 (batches=5)\n",
      "[epoch 793] mean loss = 0.912630 (batches=5)\n",
      "[epoch 794] mean loss = 0.895301 (batches=5)\n",
      "[epoch 795] mean loss = 0.882408 (batches=5)\n",
      "[epoch 796] mean loss = 0.891444 (batches=5)\n",
      "[epoch 797] mean loss = 0.905682 (batches=5)\n",
      "[epoch 798] mean loss = 0.887805 (batches=5)\n",
      "[epoch 799] mean loss = 0.827877 (batches=5)\n",
      "[epoch 800] mean loss = 0.944021 (batches=5)\n",
      "[epoch 801] mean loss = 0.813971 (batches=5)\n",
      "[epoch 802] mean loss = 0.867638 (batches=5)\n",
      "[epoch 803] mean loss = 0.904345 (batches=5)\n",
      "[epoch 804] mean loss = 0.903912 (batches=5)\n",
      "[epoch 805] mean loss = 0.877275 (batches=5)\n",
      "[epoch 806] mean loss = 0.893854 (batches=5)\n",
      "[epoch 807] mean loss = 0.901316 (batches=5)\n",
      "[epoch 808] mean loss = 0.951624 (batches=5)\n",
      "[epoch 809] mean loss = 0.907086 (batches=5)\n",
      "[epoch 810] mean loss = 0.955195 (batches=5)\n",
      "[epoch 811] mean loss = 0.910699 (batches=5)\n",
      "[epoch 812] mean loss = 0.915498 (batches=5)\n",
      "[epoch 813] mean loss = 0.945604 (batches=5)\n",
      "[epoch 814] mean loss = 0.813777 (batches=5)\n",
      "[epoch 815] mean loss = 0.894513 (batches=5)\n",
      "[epoch 816] mean loss = 0.894048 (batches=5)\n",
      "[epoch 817] mean loss = 0.967478 (batches=5)\n",
      "[epoch 818] mean loss = 0.928073 (batches=5)\n",
      "[epoch 819] mean loss = 0.913404 (batches=5)\n",
      "[epoch 820] mean loss = 0.874946 (batches=5)\n",
      "[epoch 821] mean loss = 0.952322 (batches=5)\n",
      "[epoch 822] mean loss = 0.874309 (batches=5)\n",
      "[epoch 823] mean loss = 0.899799 (batches=5)\n",
      "[epoch 824] mean loss = 0.886321 (batches=5)\n",
      "[epoch 825] mean loss = 0.934744 (batches=5)\n",
      "[epoch 826] mean loss = 0.891339 (batches=5)\n",
      "[epoch 827] mean loss = 0.880080 (batches=5)\n",
      "[epoch 828] mean loss = 0.895601 (batches=5)\n",
      "[epoch 829] mean loss = 0.891741 (batches=5)\n",
      "[epoch 830] mean loss = 0.870646 (batches=5)\n",
      "[epoch 831] mean loss = 0.878192 (batches=5)\n",
      "[epoch 832] mean loss = 0.816695 (batches=5)\n",
      "[epoch 833] mean loss = 0.892942 (batches=5)\n",
      "[epoch 834] mean loss = 0.880539 (batches=5)\n",
      "[epoch 835] mean loss = 0.839700 (batches=5)\n",
      "[epoch 836] mean loss = 0.851135 (batches=5)\n",
      "[epoch 837] mean loss = 0.897224 (batches=5)\n",
      "[epoch 838] mean loss = 0.886234 (batches=5)\n",
      "[epoch 839] mean loss = 0.880544 (batches=5)\n",
      "[epoch 840] mean loss = 0.801746 (batches=5)\n",
      "[epoch 841] mean loss = 0.803051 (batches=5)\n",
      "[epoch 842] mean loss = 0.800860 (batches=5)\n",
      "[epoch 843] mean loss = 0.869304 (batches=5)\n",
      "[epoch 844] mean loss = 0.835219 (batches=5)\n",
      "[epoch 845] mean loss = 0.849082 (batches=5)\n",
      "[epoch 846] mean loss = 0.873716 (batches=5)\n",
      "[epoch 847] mean loss = 0.895070 (batches=5)\n",
      "[epoch 848] mean loss = 0.894472 (batches=5)\n",
      "[epoch 849] mean loss = 0.927501 (batches=5)\n",
      "[epoch 850] mean loss = 0.905626 (batches=5)\n",
      "[epoch 851] mean loss = 0.792256 (batches=5)\n",
      "[epoch 852] mean loss = 0.842877 (batches=5)\n",
      "[epoch 853] mean loss = 0.855133 (batches=5)\n",
      "[epoch 854] mean loss = 0.807328 (batches=5)\n",
      "[epoch 855] mean loss = 0.842806 (batches=5)\n",
      "[epoch 856] mean loss = 0.843703 (batches=5)\n",
      "[epoch 857] mean loss = 0.823220 (batches=5)\n",
      "[epoch 858] mean loss = 0.871955 (batches=5)\n",
      "[epoch 859] mean loss = 0.930336 (batches=5)\n",
      "[epoch 860] mean loss = 0.780889 (batches=5)\n",
      "[epoch 861] mean loss = 0.849123 (batches=5)\n",
      "[epoch 862] mean loss = 0.853834 (batches=5)\n",
      "[epoch 863] mean loss = 0.842006 (batches=5)\n",
      "[epoch 864] mean loss = 0.796672 (batches=5)\n",
      "[epoch 865] mean loss = 0.832262 (batches=5)\n",
      "[epoch 866] mean loss = 0.911769 (batches=5)\n",
      "[epoch 867] mean loss = 0.849945 (batches=5)\n",
      "[epoch 868] mean loss = 0.850387 (batches=5)\n",
      "[epoch 869] mean loss = 0.809354 (batches=5)\n",
      "[epoch 870] mean loss = 0.847185 (batches=5)\n",
      "[epoch 871] mean loss = 0.846724 (batches=5)\n",
      "[epoch 872] mean loss = 0.829487 (batches=5)\n",
      "[epoch 873] mean loss = 0.832488 (batches=5)\n",
      "[epoch 874] mean loss = 0.875862 (batches=5)\n",
      "[epoch 875] mean loss = 0.851099 (batches=5)\n",
      "[epoch 876] mean loss = 0.881550 (batches=5)\n",
      "[epoch 877] mean loss = 0.823316 (batches=5)\n",
      "[epoch 878] mean loss = 0.796063 (batches=5)\n",
      "[epoch 879] mean loss = 0.826229 (batches=5)\n",
      "[epoch 880] mean loss = 0.809837 (batches=5)\n",
      "[epoch 881] mean loss = 0.824519 (batches=5)\n",
      "[epoch 882] mean loss = 0.817994 (batches=5)\n",
      "[epoch 883] mean loss = 0.805784 (batches=5)\n",
      "[epoch 884] mean loss = 0.836943 (batches=5)\n",
      "[epoch 885] mean loss = 0.808287 (batches=5)\n",
      "[epoch 886] mean loss = 0.722272 (batches=5)\n",
      "[epoch 887] mean loss = 0.787655 (batches=5)\n",
      "[epoch 888] mean loss = 0.794225 (batches=5)\n",
      "[epoch 889] mean loss = 0.834023 (batches=5)\n",
      "[epoch 890] mean loss = 0.861863 (batches=5)\n",
      "[epoch 891] mean loss = 0.763697 (batches=5)\n",
      "[epoch 892] mean loss = 0.839389 (batches=5)\n",
      "[epoch 893] mean loss = 0.841572 (batches=5)\n",
      "[epoch 894] mean loss = 0.787213 (batches=5)\n",
      "[epoch 895] mean loss = 0.778264 (batches=5)\n",
      "[epoch 896] mean loss = 0.822052 (batches=5)\n",
      "[epoch 897] mean loss = 0.713262 (batches=5)\n",
      "[epoch 898] mean loss = 0.788838 (batches=5)\n",
      "[epoch 899] mean loss = 0.824596 (batches=5)\n",
      "[epoch 900] mean loss = 0.813900 (batches=5)\n",
      "[epoch 901] mean loss = 0.855885 (batches=5)\n",
      "[epoch 902] mean loss = 0.903539 (batches=5)\n",
      "[epoch 903] mean loss = 0.880483 (batches=5)\n",
      "[epoch 904] mean loss = 0.780884 (batches=5)\n",
      "[epoch 905] mean loss = 0.754199 (batches=5)\n",
      "[epoch 906] mean loss = 0.766156 (batches=5)\n",
      "[epoch 907] mean loss = 0.820613 (batches=5)\n",
      "[epoch 908] mean loss = 0.714880 (batches=5)\n",
      "[epoch 909] mean loss = 0.766569 (batches=5)\n",
      "[epoch 910] mean loss = 0.775450 (batches=5)\n",
      "[epoch 911] mean loss = 0.822498 (batches=5)\n",
      "[epoch 912] mean loss = 0.774302 (batches=5)\n",
      "[epoch 913] mean loss = 0.741072 (batches=5)\n",
      "[epoch 914] mean loss = 0.787227 (batches=5)\n",
      "[epoch 915] mean loss = 0.742707 (batches=5)\n",
      "[epoch 916] mean loss = 0.788648 (batches=5)\n",
      "[epoch 917] mean loss = 0.844783 (batches=5)\n",
      "[epoch 918] mean loss = 0.839679 (batches=5)\n",
      "[epoch 919] mean loss = 0.800204 (batches=5)\n",
      "[epoch 920] mean loss = 0.826603 (batches=5)\n",
      "[epoch 921] mean loss = 0.798490 (batches=5)\n",
      "[epoch 922] mean loss = 0.791535 (batches=5)\n",
      "[epoch 923] mean loss = 0.740049 (batches=5)\n",
      "[epoch 924] mean loss = 0.768001 (batches=5)\n",
      "[epoch 925] mean loss = 0.789860 (batches=5)\n",
      "[epoch 926] mean loss = 0.755557 (batches=5)\n",
      "[epoch 927] mean loss = 0.776549 (batches=5)\n",
      "[epoch 928] mean loss = 0.766026 (batches=5)\n",
      "[epoch 929] mean loss = 0.834278 (batches=5)\n",
      "[epoch 930] mean loss = 0.747422 (batches=5)\n",
      "[epoch 931] mean loss = 0.752456 (batches=5)\n",
      "[epoch 932] mean loss = 0.771776 (batches=5)\n",
      "[epoch 933] mean loss = 0.781310 (batches=5)\n",
      "[epoch 934] mean loss = 0.808040 (batches=5)\n",
      "[epoch 935] mean loss = 0.719823 (batches=5)\n",
      "[epoch 936] mean loss = 0.821280 (batches=5)\n",
      "[epoch 937] mean loss = 0.776934 (batches=5)\n",
      "[epoch 938] mean loss = 0.700276 (batches=5)\n",
      "[epoch 939] mean loss = 0.811472 (batches=5)\n",
      "[epoch 940] mean loss = 0.766406 (batches=5)\n",
      "[epoch 941] mean loss = 0.736771 (batches=5)\n",
      "[epoch 942] mean loss = 0.754226 (batches=5)\n",
      "[epoch 943] mean loss = 0.751941 (batches=5)\n",
      "[epoch 944] mean loss = 0.736466 (batches=5)\n",
      "[epoch 945] mean loss = 0.757792 (batches=5)\n",
      "[epoch 946] mean loss = 0.771845 (batches=5)\n",
      "[epoch 947] mean loss = 0.744268 (batches=5)\n",
      "[epoch 948] mean loss = 0.745392 (batches=5)\n",
      "[epoch 949] mean loss = 0.758233 (batches=5)\n",
      "[epoch 950] mean loss = 0.718466 (batches=5)\n",
      "[epoch 951] mean loss = 0.749107 (batches=5)\n",
      "[epoch 952] mean loss = 0.768138 (batches=5)\n",
      "[epoch 953] mean loss = 0.761283 (batches=5)\n",
      "[epoch 954] mean loss = 0.731071 (batches=5)\n",
      "[epoch 955] mean loss = 0.812264 (batches=5)\n",
      "[epoch 956] mean loss = 0.705834 (batches=5)\n",
      "[epoch 957] mean loss = 0.728644 (batches=5)\n",
      "[epoch 958] mean loss = 0.771791 (batches=5)\n",
      "[epoch 959] mean loss = 0.757289 (batches=5)\n",
      "[epoch 960] mean loss = 0.726486 (batches=5)\n",
      "[epoch 961] mean loss = 0.744059 (batches=5)\n",
      "[epoch 962] mean loss = 0.765590 (batches=5)\n",
      "[epoch 963] mean loss = 0.739593 (batches=5)\n",
      "[epoch 964] mean loss = 0.650083 (batches=5)\n",
      "[epoch 965] mean loss = 0.722369 (batches=5)\n",
      "[epoch 966] mean loss = 0.684962 (batches=5)\n",
      "[epoch 967] mean loss = 0.731757 (batches=5)\n",
      "[epoch 968] mean loss = 0.734574 (batches=5)\n",
      "[epoch 969] mean loss = 0.751890 (batches=5)\n",
      "[epoch 970] mean loss = 0.705254 (batches=5)\n",
      "[epoch 971] mean loss = 0.732388 (batches=5)\n",
      "[epoch 972] mean loss = 0.721389 (batches=5)\n",
      "[epoch 973] mean loss = 0.759348 (batches=5)\n",
      "[epoch 974] mean loss = 0.728397 (batches=5)\n",
      "[epoch 975] mean loss = 0.771403 (batches=5)\n",
      "[epoch 976] mean loss = 0.783141 (batches=5)\n",
      "[epoch 977] mean loss = 0.741619 (batches=5)\n",
      "[epoch 978] mean loss = 0.713046 (batches=5)\n",
      "[epoch 979] mean loss = 0.698053 (batches=5)\n",
      "[epoch 980] mean loss = 0.785204 (batches=5)\n",
      "[epoch 981] mean loss = 0.691591 (batches=5)\n",
      "[epoch 982] mean loss = 0.675471 (batches=5)\n",
      "[epoch 983] mean loss = 0.698485 (batches=5)\n",
      "[epoch 984] mean loss = 0.718555 (batches=5)\n",
      "[epoch 985] mean loss = 0.703522 (batches=5)\n",
      "[epoch 986] mean loss = 0.698387 (batches=5)\n",
      "[epoch 987] mean loss = 0.743193 (batches=5)\n",
      "[epoch 988] mean loss = 0.733020 (batches=5)\n",
      "[epoch 989] mean loss = 0.748063 (batches=5)\n",
      "[epoch 990] mean loss = 0.732431 (batches=5)\n",
      "[epoch 991] mean loss = 0.623724 (batches=5)\n",
      "[epoch 992] mean loss = 0.715295 (batches=5)\n",
      "[epoch 993] mean loss = 0.707086 (batches=5)\n",
      "[epoch 994] mean loss = 0.726691 (batches=5)\n",
      "[epoch 995] mean loss = 0.710617 (batches=5)\n",
      "[epoch 996] mean loss = 0.684979 (batches=5)\n",
      "[epoch 997] mean loss = 0.697990 (batches=5)\n",
      "[epoch 998] mean loss = 0.672843 (batches=5)\n",
      "[epoch 999] mean loss = 0.720609 (batches=5)\n",
      "[epoch 1000] mean loss = 0.756702 (batches=5)\n",
      "[epoch 1001] mean loss = 0.716724 (batches=5)\n",
      "[epoch 1002] mean loss = 0.721775 (batches=5)\n",
      "[epoch 1003] mean loss = 0.733023 (batches=5)\n",
      "[epoch 1004] mean loss = 0.677312 (batches=5)\n",
      "[epoch 1005] mean loss = 0.664303 (batches=5)\n",
      "[epoch 1006] mean loss = 0.722366 (batches=5)\n",
      "[epoch 1007] mean loss = 0.686013 (batches=5)\n",
      "[epoch 1008] mean loss = 0.724497 (batches=5)\n",
      "[epoch 1009] mean loss = 0.769763 (batches=5)\n",
      "[epoch 1010] mean loss = 0.755611 (batches=5)\n",
      "[epoch 1011] mean loss = 0.676921 (batches=5)\n",
      "[epoch 1012] mean loss = 0.694934 (batches=5)\n",
      "[epoch 1013] mean loss = 0.706445 (batches=5)\n",
      "[epoch 1014] mean loss = 0.721160 (batches=5)\n",
      "[epoch 1015] mean loss = 0.733817 (batches=5)\n",
      "[epoch 1016] mean loss = 0.703331 (batches=5)\n",
      "[epoch 1017] mean loss = 0.719561 (batches=5)\n",
      "[epoch 1018] mean loss = 0.753185 (batches=5)\n",
      "[epoch 1019] mean loss = 0.730279 (batches=5)\n",
      "[epoch 1020] mean loss = 0.728847 (batches=5)\n",
      "[epoch 1021] mean loss = 0.650962 (batches=5)\n",
      "[epoch 1022] mean loss = 0.650187 (batches=5)\n",
      "[epoch 1023] mean loss = 0.709670 (batches=5)\n",
      "[epoch 1024] mean loss = 0.701265 (batches=5)\n",
      "[epoch 1025] mean loss = 0.731469 (batches=5)\n",
      "[epoch 1026] mean loss = 0.641419 (batches=5)\n",
      "[epoch 1027] mean loss = 0.706772 (batches=5)\n",
      "[epoch 1028] mean loss = 0.660189 (batches=5)\n",
      "[epoch 1029] mean loss = 0.707902 (batches=5)\n",
      "[epoch 1030] mean loss = 0.748524 (batches=5)\n",
      "[epoch 1031] mean loss = 0.713640 (batches=5)\n",
      "[epoch 1032] mean loss = 0.697306 (batches=5)\n",
      "[epoch 1033] mean loss = 0.710434 (batches=5)\n",
      "[epoch 1034] mean loss = 0.662104 (batches=5)\n",
      "[epoch 1035] mean loss = 0.741531 (batches=5)\n",
      "[epoch 1036] mean loss = 0.675426 (batches=5)\n",
      "[epoch 1037] mean loss = 0.719888 (batches=5)\n",
      "[epoch 1038] mean loss = 0.687698 (batches=5)\n",
      "[epoch 1039] mean loss = 0.706739 (batches=5)\n",
      "[epoch 1040] mean loss = 0.695236 (batches=5)\n",
      "[epoch 1041] mean loss = 0.653054 (batches=5)\n",
      "[epoch 1042] mean loss = 0.682166 (batches=5)\n",
      "[epoch 1043] mean loss = 0.661002 (batches=5)\n",
      "[epoch 1044] mean loss = 0.698225 (batches=5)\n",
      "[epoch 1045] mean loss = 0.719185 (batches=5)\n",
      "[epoch 1046] mean loss = 0.681210 (batches=5)\n",
      "[epoch 1047] mean loss = 0.706472 (batches=5)\n",
      "[epoch 1048] mean loss = 0.706770 (batches=5)\n",
      "[epoch 1049] mean loss = 0.650587 (batches=5)\n",
      "[epoch 1050] mean loss = 0.676172 (batches=5)\n",
      "[epoch 1051] mean loss = 0.641299 (batches=5)\n",
      "[epoch 1052] mean loss = 0.666262 (batches=5)\n",
      "[epoch 1053] mean loss = 0.684372 (batches=5)\n",
      "[epoch 1054] mean loss = 0.683972 (batches=5)\n",
      "[epoch 1055] mean loss = 0.675739 (batches=5)\n",
      "[epoch 1056] mean loss = 0.667215 (batches=5)\n",
      "[epoch 1057] mean loss = 0.659694 (batches=5)\n",
      "[epoch 1058] mean loss = 0.659390 (batches=5)\n",
      "[epoch 1059] mean loss = 0.674099 (batches=5)\n",
      "[epoch 1060] mean loss = 0.705377 (batches=5)\n",
      "[epoch 1061] mean loss = 0.648702 (batches=5)\n",
      "[epoch 1062] mean loss = 0.703359 (batches=5)\n",
      "[epoch 1063] mean loss = 0.625858 (batches=5)\n",
      "[epoch 1064] mean loss = 0.697994 (batches=5)\n",
      "[epoch 1065] mean loss = 0.583271 (batches=5)\n",
      "[epoch 1066] mean loss = 0.622408 (batches=5)\n",
      "[epoch 1067] mean loss = 0.678381 (batches=5)\n",
      "[epoch 1068] mean loss = 0.735831 (batches=5)\n",
      "[epoch 1069] mean loss = 0.632180 (batches=5)\n",
      "[epoch 1070] mean loss = 0.644398 (batches=5)\n",
      "[epoch 1071] mean loss = 0.686349 (batches=5)\n",
      "[epoch 1072] mean loss = 0.649329 (batches=5)\n",
      "[epoch 1073] mean loss = 0.666690 (batches=5)\n",
      "[epoch 1074] mean loss = 0.647063 (batches=5)\n",
      "[epoch 1075] mean loss = 0.681926 (batches=5)\n",
      "[epoch 1076] mean loss = 0.607008 (batches=5)\n",
      "[epoch 1077] mean loss = 0.659560 (batches=5)\n",
      "[epoch 1078] mean loss = 0.703046 (batches=5)\n",
      "[epoch 1079] mean loss = 0.634411 (batches=5)\n",
      "[epoch 1080] mean loss = 0.667669 (batches=5)\n",
      "[epoch 1081] mean loss = 0.663225 (batches=5)\n",
      "[epoch 1082] mean loss = 0.636193 (batches=5)\n",
      "[epoch 1083] mean loss = 0.654800 (batches=5)\n",
      "[epoch 1084] mean loss = 0.676567 (batches=5)\n",
      "[epoch 1085] mean loss = 0.656286 (batches=5)\n",
      "[epoch 1086] mean loss = 0.645738 (batches=5)\n",
      "[epoch 1087] mean loss = 0.633052 (batches=5)\n",
      "[epoch 1088] mean loss = 0.644311 (batches=5)\n",
      "[epoch 1089] mean loss = 0.652290 (batches=5)\n",
      "[epoch 1090] mean loss = 0.667329 (batches=5)\n",
      "[epoch 1091] mean loss = 0.698073 (batches=5)\n",
      "[epoch 1092] mean loss = 0.662545 (batches=5)\n",
      "[epoch 1093] mean loss = 0.708705 (batches=5)\n",
      "[epoch 1094] mean loss = 0.599844 (batches=5)\n",
      "[epoch 1095] mean loss = 0.614290 (batches=5)\n",
      "[epoch 1096] mean loss = 0.588104 (batches=5)\n",
      "[epoch 1097] mean loss = 0.610979 (batches=5)\n",
      "[epoch 1098] mean loss = 0.593810 (batches=5)\n",
      "[epoch 1099] mean loss = 0.643208 (batches=5)\n",
      "[epoch 1100] mean loss = 0.648429 (batches=5)\n",
      "[epoch 1101] mean loss = 0.657539 (batches=5)\n",
      "[epoch 1102] mean loss = 0.666566 (batches=5)\n",
      "[epoch 1103] mean loss = 0.612585 (batches=5)\n",
      "[epoch 1104] mean loss = 0.673299 (batches=5)\n",
      "[epoch 1105] mean loss = 0.613247 (batches=5)\n",
      "[epoch 1106] mean loss = 0.616281 (batches=5)\n",
      "[epoch 1107] mean loss = 0.633748 (batches=5)\n",
      "[epoch 1108] mean loss = 0.571303 (batches=5)\n",
      "[epoch 1109] mean loss = 0.627253 (batches=5)\n",
      "[epoch 1110] mean loss = 0.592030 (batches=5)\n",
      "[epoch 1111] mean loss = 0.629465 (batches=5)\n",
      "[epoch 1112] mean loss = 0.622011 (batches=5)\n",
      "[epoch 1113] mean loss = 0.618321 (batches=5)\n",
      "[epoch 1114] mean loss = 0.606288 (batches=5)\n",
      "[epoch 1115] mean loss = 0.631592 (batches=5)\n",
      "[epoch 1116] mean loss = 0.605348 (batches=5)\n",
      "[epoch 1117] mean loss = 0.602616 (batches=5)\n",
      "[epoch 1118] mean loss = 0.620771 (batches=5)\n",
      "[epoch 1119] mean loss = 0.606733 (batches=5)\n",
      "[epoch 1120] mean loss = 0.628840 (batches=5)\n",
      "[epoch 1121] mean loss = 0.591660 (batches=5)\n",
      "[epoch 1122] mean loss = 0.584627 (batches=5)\n",
      "[epoch 1123] mean loss = 0.582643 (batches=5)\n",
      "[epoch 1124] mean loss = 0.571772 (batches=5)\n",
      "[epoch 1125] mean loss = 0.600418 (batches=5)\n",
      "[epoch 1126] mean loss = 0.582647 (batches=5)\n",
      "[epoch 1127] mean loss = 0.665415 (batches=5)\n",
      "[epoch 1128] mean loss = 0.546598 (batches=5)\n",
      "[epoch 1129] mean loss = 0.575269 (batches=5)\n",
      "[epoch 1130] mean loss = 0.642782 (batches=5)\n",
      "[epoch 1131] mean loss = 0.571903 (batches=5)\n",
      "[epoch 1132] mean loss = 0.636607 (batches=5)\n",
      "[epoch 1133] mean loss = 0.610719 (batches=5)\n",
      "[epoch 1134] mean loss = 0.622941 (batches=5)\n",
      "[epoch 1135] mean loss = 0.538308 (batches=5)\n",
      "[epoch 1136] mean loss = 0.604041 (batches=5)\n",
      "[epoch 1137] mean loss = 0.593415 (batches=5)\n",
      "[epoch 1138] mean loss = 0.576024 (batches=5)\n",
      "[epoch 1139] mean loss = 0.580201 (batches=5)\n",
      "[epoch 1140] mean loss = 0.630778 (batches=5)\n",
      "[epoch 1141] mean loss = 0.594759 (batches=5)\n",
      "[epoch 1142] mean loss = 0.532818 (batches=5)\n",
      "[epoch 1143] mean loss = 0.590767 (batches=5)\n",
      "[epoch 1144] mean loss = 0.583649 (batches=5)\n",
      "[epoch 1145] mean loss = 0.636628 (batches=5)\n",
      "[epoch 1146] mean loss = 0.611614 (batches=5)\n",
      "[epoch 1147] mean loss = 0.615092 (batches=5)\n",
      "[epoch 1148] mean loss = 0.590663 (batches=5)\n",
      "[epoch 1149] mean loss = 0.568788 (batches=5)\n",
      "[epoch 1150] mean loss = 0.570673 (batches=5)\n",
      "[epoch 1151] mean loss = 0.528563 (batches=5)\n",
      "[epoch 1152] mean loss = 0.595295 (batches=5)\n",
      "[epoch 1153] mean loss = 0.596285 (batches=5)\n",
      "[epoch 1154] mean loss = 0.575831 (batches=5)\n",
      "[epoch 1155] mean loss = 0.605867 (batches=5)\n",
      "[epoch 1156] mean loss = 0.588567 (batches=5)\n",
      "[epoch 1157] mean loss = 0.556800 (batches=5)\n",
      "[epoch 1158] mean loss = 0.560618 (batches=5)\n",
      "[epoch 1159] mean loss = 0.624094 (batches=5)\n",
      "[epoch 1160] mean loss = 0.621836 (batches=5)\n",
      "[epoch 1161] mean loss = 0.589257 (batches=5)\n",
      "[epoch 1162] mean loss = 0.551700 (batches=5)\n",
      "[epoch 1163] mean loss = 0.567463 (batches=5)\n",
      "[epoch 1164] mean loss = 0.618383 (batches=5)\n",
      "[epoch 1165] mean loss = 0.597487 (batches=5)\n",
      "[epoch 1166] mean loss = 0.608793 (batches=5)\n",
      "[epoch 1167] mean loss = 0.581662 (batches=5)\n",
      "[epoch 1168] mean loss = 0.576552 (batches=5)\n",
      "[epoch 1169] mean loss = 0.596431 (batches=5)\n",
      "[epoch 1170] mean loss = 0.541397 (batches=5)\n",
      "[epoch 1171] mean loss = 0.529297 (batches=5)\n",
      "[epoch 1172] mean loss = 0.572513 (batches=5)\n",
      "[epoch 1173] mean loss = 0.569511 (batches=5)\n",
      "[epoch 1174] mean loss = 0.624683 (batches=5)\n",
      "[epoch 1175] mean loss = 0.577307 (batches=5)\n",
      "[epoch 1176] mean loss = 0.600272 (batches=5)\n",
      "[epoch 1177] mean loss = 0.560525 (batches=5)\n",
      "[epoch 1178] mean loss = 0.584195 (batches=5)\n",
      "[epoch 1179] mean loss = 0.586920 (batches=5)\n",
      "[epoch 1180] mean loss = 0.591643 (batches=5)\n",
      "[epoch 1181] mean loss = 0.594747 (batches=5)\n",
      "[epoch 1182] mean loss = 0.535906 (batches=5)\n",
      "[epoch 1183] mean loss = 0.587252 (batches=5)\n",
      "[epoch 1184] mean loss = 0.507450 (batches=5)\n",
      "[epoch 1185] mean loss = 0.588539 (batches=5)\n",
      "[epoch 1186] mean loss = 0.570554 (batches=5)\n",
      "[epoch 1187] mean loss = 0.531518 (batches=5)\n",
      "[epoch 1188] mean loss = 0.544562 (batches=5)\n",
      "[epoch 1189] mean loss = 0.559405 (batches=5)\n",
      "[epoch 1190] mean loss = 0.556702 (batches=5)\n",
      "[epoch 1191] mean loss = 0.562499 (batches=5)\n",
      "[epoch 1192] mean loss = 0.548661 (batches=5)\n",
      "[epoch 1193] mean loss = 0.569592 (batches=5)\n",
      "[epoch 1194] mean loss = 0.587549 (batches=5)\n",
      "[epoch 1195] mean loss = 0.580202 (batches=5)\n",
      "[epoch 1196] mean loss = 0.550673 (batches=5)\n",
      "[epoch 1197] mean loss = 0.548141 (batches=5)\n",
      "[epoch 1198] mean loss = 0.520405 (batches=5)\n",
      "[epoch 1199] mean loss = 0.542901 (batches=5)\n",
      "[epoch 1200] mean loss = 0.524024 (batches=5)\n",
      "[epoch 1201] mean loss = 0.568055 (batches=5)\n",
      "[epoch 1202] mean loss = 0.565907 (batches=5)\n",
      "[epoch 1203] mean loss = 0.561704 (batches=5)\n",
      "[epoch 1204] mean loss = 0.574214 (batches=5)\n",
      "[epoch 1205] mean loss = 0.550047 (batches=5)\n",
      "[epoch 1206] mean loss = 0.483600 (batches=5)\n",
      "[epoch 1207] mean loss = 0.570884 (batches=5)\n",
      "[epoch 1208] mean loss = 0.574314 (batches=5)\n",
      "[epoch 1209] mean loss = 0.543410 (batches=5)\n",
      "[epoch 1210] mean loss = 0.536528 (batches=5)\n",
      "[epoch 1211] mean loss = 0.528283 (batches=5)\n",
      "[epoch 1212] mean loss = 0.523155 (batches=5)\n",
      "[epoch 1213] mean loss = 0.527724 (batches=5)\n",
      "[epoch 1214] mean loss = 0.540460 (batches=5)\n",
      "[epoch 1215] mean loss = 0.560681 (batches=5)\n",
      "[epoch 1216] mean loss = 0.454197 (batches=5)\n",
      "[epoch 1217] mean loss = 0.525374 (batches=5)\n",
      "[epoch 1218] mean loss = 0.539569 (batches=5)\n",
      "[epoch 1219] mean loss = 0.511170 (batches=5)\n",
      "[epoch 1220] mean loss = 0.610375 (batches=5)\n",
      "[epoch 1221] mean loss = 0.524732 (batches=5)\n",
      "[epoch 1222] mean loss = 0.527276 (batches=5)\n",
      "[epoch 1223] mean loss = 0.539594 (batches=5)\n",
      "[epoch 1224] mean loss = 0.568212 (batches=5)\n",
      "[epoch 1225] mean loss = 0.532684 (batches=5)\n",
      "[epoch 1226] mean loss = 0.517152 (batches=5)\n",
      "[epoch 1227] mean loss = 0.552749 (batches=5)\n",
      "[epoch 1228] mean loss = 0.521834 (batches=5)\n",
      "[epoch 1229] mean loss = 0.508933 (batches=5)\n",
      "[epoch 1230] mean loss = 0.505139 (batches=5)\n",
      "[epoch 1231] mean loss = 0.498122 (batches=5)\n",
      "[epoch 1232] mean loss = 0.490961 (batches=5)\n",
      "[epoch 1233] mean loss = 0.542917 (batches=5)\n",
      "[epoch 1234] mean loss = 0.512326 (batches=5)\n",
      "[epoch 1235] mean loss = 0.528933 (batches=5)\n",
      "[epoch 1236] mean loss = 0.477096 (batches=5)\n",
      "[epoch 1237] mean loss = 0.514334 (batches=5)\n",
      "[epoch 1238] mean loss = 0.505304 (batches=5)\n",
      "[epoch 1239] mean loss = 0.532143 (batches=5)\n",
      "[epoch 1240] mean loss = 0.508214 (batches=5)\n",
      "[epoch 1241] mean loss = 0.497120 (batches=5)\n",
      "[epoch 1242] mean loss = 0.489674 (batches=5)\n",
      "[epoch 1243] mean loss = 0.524760 (batches=5)\n",
      "[epoch 1244] mean loss = 0.492918 (batches=5)\n",
      "[epoch 1245] mean loss = 0.512003 (batches=5)\n",
      "[epoch 1246] mean loss = 0.534151 (batches=5)\n",
      "[epoch 1247] mean loss = 0.465107 (batches=5)\n",
      "[epoch 1248] mean loss = 0.482549 (batches=5)\n",
      "[epoch 1249] mean loss = 0.509900 (batches=5)\n",
      "[epoch 1250] mean loss = 0.524688 (batches=5)\n",
      "[epoch 1251] mean loss = 0.507979 (batches=5)\n",
      "[epoch 1252] mean loss = 0.519677 (batches=5)\n",
      "[epoch 1253] mean loss = 0.507272 (batches=5)\n",
      "[epoch 1254] mean loss = 0.502018 (batches=5)\n",
      "[epoch 1255] mean loss = 0.497318 (batches=5)\n",
      "[epoch 1256] mean loss = 0.510081 (batches=5)\n",
      "[epoch 1257] mean loss = 0.561456 (batches=5)\n",
      "[epoch 1258] mean loss = 0.481482 (batches=5)\n",
      "[epoch 1259] mean loss = 0.478393 (batches=5)\n",
      "[epoch 1260] mean loss = 0.494722 (batches=5)\n",
      "[epoch 1261] mean loss = 0.536709 (batches=5)\n",
      "[epoch 1262] mean loss = 0.507832 (batches=5)\n",
      "[epoch 1263] mean loss = 0.491116 (batches=5)\n",
      "[epoch 1264] mean loss = 0.457145 (batches=5)\n",
      "[epoch 1265] mean loss = 0.475714 (batches=5)\n",
      "[epoch 1266] mean loss = 0.478350 (batches=5)\n",
      "[epoch 1267] mean loss = 0.464817 (batches=5)\n",
      "[epoch 1268] mean loss = 0.489698 (batches=5)\n",
      "[epoch 1269] mean loss = 0.484323 (batches=5)\n",
      "[epoch 1270] mean loss = 0.460974 (batches=5)\n",
      "[epoch 1271] mean loss = 0.530054 (batches=5)\n",
      "[epoch 1272] mean loss = 0.497206 (batches=5)\n",
      "[epoch 1273] mean loss = 0.523633 (batches=5)\n",
      "[epoch 1274] mean loss = 0.522866 (batches=5)\n",
      "[epoch 1275] mean loss = 0.502268 (batches=5)\n",
      "[epoch 1276] mean loss = 0.482633 (batches=5)\n",
      "[epoch 1277] mean loss = 0.471129 (batches=5)\n",
      "[epoch 1278] mean loss = 0.447454 (batches=5)\n",
      "[epoch 1279] mean loss = 0.420376 (batches=5)\n",
      "[epoch 1280] mean loss = 0.509109 (batches=5)\n",
      "[epoch 1281] mean loss = 0.517025 (batches=5)\n",
      "[epoch 1282] mean loss = 0.487895 (batches=5)\n",
      "[epoch 1283] mean loss = 0.527934 (batches=5)\n",
      "[epoch 1284] mean loss = 0.498806 (batches=5)\n",
      "[epoch 1285] mean loss = 0.483077 (batches=5)\n",
      "[epoch 1286] mean loss = 0.489520 (batches=5)\n",
      "[epoch 1287] mean loss = 0.477082 (batches=5)\n",
      "[epoch 1288] mean loss = 0.492017 (batches=5)\n",
      "[epoch 1289] mean loss = 0.483073 (batches=5)\n",
      "[epoch 1290] mean loss = 0.520994 (batches=5)\n",
      "[epoch 1291] mean loss = 0.494420 (batches=5)\n",
      "[epoch 1292] mean loss = 0.460453 (batches=5)\n",
      "[epoch 1293] mean loss = 0.465793 (batches=5)\n",
      "[epoch 1294] mean loss = 0.509324 (batches=5)\n",
      "[epoch 1295] mean loss = 0.498253 (batches=5)\n",
      "[epoch 1296] mean loss = 0.468863 (batches=5)\n",
      "[epoch 1297] mean loss = 0.445251 (batches=5)\n",
      "[epoch 1298] mean loss = 0.451082 (batches=5)\n",
      "[epoch 1299] mean loss = 0.446884 (batches=5)\n",
      "[epoch 1300] mean loss = 0.414132 (batches=5)\n",
      "[epoch 1301] mean loss = 0.493813 (batches=5)\n",
      "[epoch 1302] mean loss = 0.466329 (batches=5)\n",
      "[epoch 1303] mean loss = 0.450650 (batches=5)\n",
      "[epoch 1304] mean loss = 0.528404 (batches=5)\n",
      "[epoch 1305] mean loss = 0.499250 (batches=5)\n",
      "[epoch 1306] mean loss = 0.472957 (batches=5)\n",
      "[epoch 1307] mean loss = 0.488163 (batches=5)\n",
      "[epoch 1308] mean loss = 0.506382 (batches=5)\n",
      "[epoch 1309] mean loss = 0.487139 (batches=5)\n",
      "[epoch 1310] mean loss = 0.478779 (batches=5)\n",
      "[epoch 1311] mean loss = 0.465292 (batches=5)\n",
      "[epoch 1312] mean loss = 0.460106 (batches=5)\n",
      "[epoch 1313] mean loss = 0.480692 (batches=5)\n",
      "[epoch 1314] mean loss = 0.444633 (batches=5)\n",
      "[epoch 1315] mean loss = 0.440392 (batches=5)\n",
      "[epoch 1316] mean loss = 0.421037 (batches=5)\n",
      "[epoch 1317] mean loss = 0.456815 (batches=5)\n",
      "[epoch 1318] mean loss = 0.467737 (batches=5)\n",
      "[epoch 1319] mean loss = 0.473599 (batches=5)\n",
      "[epoch 1320] mean loss = 0.429545 (batches=5)\n",
      "[epoch 1321] mean loss = 0.439440 (batches=5)\n",
      "[epoch 1322] mean loss = 0.463521 (batches=5)\n",
      "[epoch 1323] mean loss = 0.456541 (batches=5)\n",
      "[epoch 1324] mean loss = 0.449724 (batches=5)\n",
      "[epoch 1325] mean loss = 0.417062 (batches=5)\n",
      "[epoch 1326] mean loss = 0.458238 (batches=5)\n",
      "[epoch 1327] mean loss = 0.429491 (batches=5)\n",
      "[epoch 1328] mean loss = 0.425858 (batches=5)\n",
      "[epoch 1329] mean loss = 0.459977 (batches=5)\n",
      "[epoch 1330] mean loss = 0.464931 (batches=5)\n",
      "[epoch 1331] mean loss = 0.424320 (batches=5)\n",
      "[epoch 1332] mean loss = 0.440858 (batches=5)\n",
      "[epoch 1333] mean loss = 0.442611 (batches=5)\n",
      "[epoch 1334] mean loss = 0.459493 (batches=5)\n",
      "[epoch 1335] mean loss = 0.433970 (batches=5)\n",
      "[epoch 1336] mean loss = 0.409739 (batches=5)\n",
      "[epoch 1337] mean loss = 0.417616 (batches=5)\n",
      "[epoch 1338] mean loss = 0.453741 (batches=5)\n",
      "[epoch 1339] mean loss = 0.433726 (batches=5)\n",
      "[epoch 1340] mean loss = 0.453974 (batches=5)\n",
      "[epoch 1341] mean loss = 0.485156 (batches=5)\n",
      "[epoch 1342] mean loss = 0.434658 (batches=5)\n",
      "[epoch 1343] mean loss = 0.430137 (batches=5)\n",
      "[epoch 1344] mean loss = 0.418830 (batches=5)\n",
      "[epoch 1345] mean loss = 0.442031 (batches=5)\n",
      "[epoch 1346] mean loss = 0.421283 (batches=5)\n",
      "[epoch 1347] mean loss = 0.442270 (batches=5)\n",
      "[epoch 1348] mean loss = 0.410536 (batches=5)\n",
      "[epoch 1349] mean loss = 0.425653 (batches=5)\n",
      "[epoch 1350] mean loss = 0.419299 (batches=5)\n",
      "[epoch 1351] mean loss = 0.455504 (batches=5)\n",
      "[epoch 1352] mean loss = 0.425372 (batches=5)\n",
      "[epoch 1353] mean loss = 0.428821 (batches=5)\n",
      "[epoch 1354] mean loss = 0.420324 (batches=5)\n",
      "[epoch 1355] mean loss = 0.401817 (batches=5)\n",
      "[epoch 1356] mean loss = 0.403154 (batches=5)\n",
      "[epoch 1357] mean loss = 0.440066 (batches=5)\n",
      "[epoch 1358] mean loss = 0.415207 (batches=5)\n",
      "[epoch 1359] mean loss = 0.434777 (batches=5)\n",
      "[epoch 1360] mean loss = 0.397826 (batches=5)\n",
      "[epoch 1361] mean loss = 0.437722 (batches=5)\n",
      "[epoch 1362] mean loss = 0.419846 (batches=5)\n",
      "[epoch 1363] mean loss = 0.404342 (batches=5)\n",
      "[epoch 1364] mean loss = 0.397607 (batches=5)\n",
      "[epoch 1365] mean loss = 0.402790 (batches=5)\n",
      "[epoch 1366] mean loss = 0.427661 (batches=5)\n",
      "[epoch 1367] mean loss = 0.418753 (batches=5)\n",
      "[epoch 1368] mean loss = 0.437242 (batches=5)\n",
      "[epoch 1369] mean loss = 0.459518 (batches=5)\n",
      "[epoch 1370] mean loss = 0.457652 (batches=5)\n",
      "[epoch 1371] mean loss = 0.426687 (batches=5)\n",
      "[epoch 1372] mean loss = 0.465365 (batches=5)\n",
      "[epoch 1373] mean loss = 0.437592 (batches=5)\n",
      "[epoch 1374] mean loss = 0.408671 (batches=5)\n",
      "[epoch 1375] mean loss = 0.453634 (batches=5)\n",
      "[epoch 1376] mean loss = 0.426804 (batches=5)\n",
      "[epoch 1377] mean loss = 0.407171 (batches=5)\n",
      "[epoch 1378] mean loss = 0.429098 (batches=5)\n",
      "[epoch 1379] mean loss = 0.399910 (batches=5)\n",
      "[epoch 1380] mean loss = 0.415172 (batches=5)\n",
      "[epoch 1381] mean loss = 0.424548 (batches=5)\n",
      "[epoch 1382] mean loss = 0.415610 (batches=5)\n",
      "[epoch 1383] mean loss = 0.425703 (batches=5)\n",
      "[epoch 1384] mean loss = 0.387804 (batches=5)\n",
      "[epoch 1385] mean loss = 0.421221 (batches=5)\n",
      "[epoch 1386] mean loss = 0.450193 (batches=5)\n",
      "[epoch 1387] mean loss = 0.444194 (batches=5)\n",
      "[epoch 1388] mean loss = 0.430764 (batches=5)\n",
      "[epoch 1389] mean loss = 0.500661 (batches=5)\n",
      "[epoch 1390] mean loss = 0.455971 (batches=5)\n",
      "[epoch 1391] mean loss = 0.398259 (batches=5)\n",
      "[epoch 1392] mean loss = 0.409086 (batches=5)\n",
      "[epoch 1393] mean loss = 0.423312 (batches=5)\n",
      "[epoch 1394] mean loss = 0.478742 (batches=5)\n",
      "[epoch 1395] mean loss = 0.429936 (batches=5)\n",
      "[epoch 1396] mean loss = 0.444107 (batches=5)\n",
      "[epoch 1397] mean loss = 0.503628 (batches=5)\n",
      "[epoch 1398] mean loss = 0.464127 (batches=5)\n",
      "[epoch 1399] mean loss = 0.457192 (batches=5)\n",
      "[epoch 1400] mean loss = 0.484550 (batches=5)\n",
      "[epoch 1401] mean loss = 0.485248 (batches=5)\n",
      "[epoch 1402] mean loss = 0.413071 (batches=5)\n",
      "[epoch 1403] mean loss = 0.440818 (batches=5)\n",
      "[epoch 1404] mean loss = 0.459434 (batches=5)\n",
      "[epoch 1405] mean loss = 0.444287 (batches=5)\n",
      "[epoch 1406] mean loss = 0.405715 (batches=5)\n",
      "[epoch 1407] mean loss = 0.432114 (batches=5)\n",
      "[epoch 1408] mean loss = 0.377714 (batches=5)\n",
      "[epoch 1409] mean loss = 0.399773 (batches=5)\n",
      "[epoch 1410] mean loss = 0.445723 (batches=5)\n",
      "[epoch 1411] mean loss = 0.445520 (batches=5)\n",
      "[epoch 1412] mean loss = 0.435960 (batches=5)\n",
      "[epoch 1413] mean loss = 0.445800 (batches=5)\n",
      "[epoch 1414] mean loss = 0.411584 (batches=5)\n",
      "[epoch 1415] mean loss = 0.431805 (batches=5)\n",
      "[epoch 1416] mean loss = 0.432628 (batches=5)\n",
      "[epoch 1417] mean loss = 0.438113 (batches=5)\n",
      "[epoch 1418] mean loss = 0.455024 (batches=5)\n",
      "[epoch 1419] mean loss = 0.476303 (batches=5)\n",
      "[epoch 1420] mean loss = 0.459007 (batches=5)\n",
      "[epoch 1421] mean loss = 0.427826 (batches=5)\n",
      "[epoch 1422] mean loss = 0.439345 (batches=5)\n",
      "[epoch 1423] mean loss = 0.422751 (batches=5)\n",
      "[epoch 1424] mean loss = 0.414084 (batches=5)\n",
      "[epoch 1425] mean loss = 0.431643 (batches=5)\n",
      "[epoch 1426] mean loss = 0.377659 (batches=5)\n",
      "[epoch 1427] mean loss = 0.414734 (batches=5)\n",
      "[epoch 1428] mean loss = 0.372250 (batches=5)\n",
      "[epoch 1429] mean loss = 0.431813 (batches=5)\n",
      "[epoch 1430] mean loss = 0.440185 (batches=5)\n",
      "[epoch 1431] mean loss = 0.413331 (batches=5)\n",
      "[epoch 1432] mean loss = 0.439788 (batches=5)\n",
      "[epoch 1433] mean loss = 0.453030 (batches=5)\n",
      "[epoch 1434] mean loss = 0.413533 (batches=5)\n",
      "[epoch 1435] mean loss = 0.400764 (batches=5)\n",
      "[epoch 1436] mean loss = 0.433967 (batches=5)\n",
      "[epoch 1437] mean loss = 0.414357 (batches=5)\n",
      "[epoch 1438] mean loss = 0.395241 (batches=5)\n",
      "[epoch 1439] mean loss = 0.403064 (batches=5)\n",
      "[epoch 1440] mean loss = 0.404636 (batches=5)\n",
      "[epoch 1441] mean loss = 0.391751 (batches=5)\n",
      "[epoch 1442] mean loss = 0.411272 (batches=5)\n",
      "[epoch 1443] mean loss = 0.360730 (batches=5)\n",
      "[epoch 1444] mean loss = 0.401758 (batches=5)\n",
      "[epoch 1445] mean loss = 0.406379 (batches=5)\n",
      "[epoch 1446] mean loss = 0.441487 (batches=5)\n",
      "[epoch 1447] mean loss = 0.411323 (batches=5)\n",
      "[epoch 1448] mean loss = 0.393671 (batches=5)\n",
      "[epoch 1449] mean loss = 0.422074 (batches=5)\n",
      "[epoch 1450] mean loss = 0.421763 (batches=5)\n",
      "[epoch 1451] mean loss = 0.403872 (batches=5)\n",
      "[epoch 1452] mean loss = 0.439723 (batches=5)\n",
      "[epoch 1453] mean loss = 0.391555 (batches=5)\n",
      "[epoch 1454] mean loss = 0.451914 (batches=5)\n",
      "[epoch 1455] mean loss = 0.368416 (batches=5)\n",
      "[epoch 1456] mean loss = 0.434273 (batches=5)\n",
      "[epoch 1457] mean loss = 0.387118 (batches=5)\n",
      "[epoch 1458] mean loss = 0.418410 (batches=5)\n",
      "[epoch 1459] mean loss = 0.348442 (batches=5)\n",
      "[epoch 1460] mean loss = 0.387684 (batches=5)\n",
      "[epoch 1461] mean loss = 0.402617 (batches=5)\n",
      "[epoch 1462] mean loss = 0.427695 (batches=5)\n",
      "[epoch 1463] mean loss = 0.383835 (batches=5)\n",
      "[epoch 1464] mean loss = 0.364208 (batches=5)\n",
      "[epoch 1465] mean loss = 0.368996 (batches=5)\n",
      "[epoch 1466] mean loss = 0.396359 (batches=5)\n",
      "[epoch 1467] mean loss = 0.353402 (batches=5)\n",
      "[epoch 1468] mean loss = 0.412458 (batches=5)\n",
      "[epoch 1469] mean loss = 0.388933 (batches=5)\n",
      "[epoch 1470] mean loss = 0.387491 (batches=5)\n",
      "[epoch 1471] mean loss = 0.409522 (batches=5)\n",
      "[epoch 1472] mean loss = 0.405825 (batches=5)\n",
      "[epoch 1473] mean loss = 0.406034 (batches=5)\n",
      "[epoch 1474] mean loss = 0.371599 (batches=5)\n",
      "[epoch 1475] mean loss = 0.381176 (batches=5)\n",
      "[epoch 1476] mean loss = 0.401933 (batches=5)\n",
      "[epoch 1477] mean loss = 0.367340 (batches=5)\n",
      "[epoch 1478] mean loss = 0.337903 (batches=5)\n",
      "[epoch 1479] mean loss = 0.401544 (batches=5)\n",
      "[epoch 1480] mean loss = 0.385806 (batches=5)\n",
      "[epoch 1481] mean loss = 0.412486 (batches=5)\n",
      "[epoch 1482] mean loss = 0.407058 (batches=5)\n",
      "[epoch 1483] mean loss = 0.417927 (batches=5)\n",
      "[epoch 1484] mean loss = 0.461014 (batches=5)\n",
      "[epoch 1485] mean loss = 0.381185 (batches=5)\n",
      "[epoch 1486] mean loss = 0.385523 (batches=5)\n",
      "[epoch 1487] mean loss = 0.449730 (batches=5)\n",
      "[epoch 1488] mean loss = 0.419254 (batches=5)\n",
      "[epoch 1489] mean loss = 0.427565 (batches=5)\n",
      "[epoch 1490] mean loss = 0.380333 (batches=5)\n",
      "[epoch 1491] mean loss = 0.413366 (batches=5)\n",
      "[epoch 1492] mean loss = 0.401994 (batches=5)\n",
      "[epoch 1493] mean loss = 0.406972 (batches=5)\n",
      "[epoch 1494] mean loss = 0.383772 (batches=5)\n",
      "[epoch 1495] mean loss = 0.378438 (batches=5)\n",
      "[epoch 1496] mean loss = 0.386384 (batches=5)\n",
      "[epoch 1497] mean loss = 0.383865 (batches=5)\n",
      "[epoch 1498] mean loss = 0.368483 (batches=5)\n",
      "[epoch 1499] mean loss = 0.354265 (batches=5)\n",
      "[epoch 1500] mean loss = 0.369638 (batches=5)\n",
      "[done] training finished. logged 1500 loss points.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:23:41.385237Z",
     "start_time": "2025-12-12T20:23:41.343082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============ 3) inference ============\n",
    "# training set\n",
    "with torch.no_grad():\n",
    "    Xm_train = X_train_t.mean(dim=1).to(device)  # [N, T]\n",
    "    h_train, u_train = model(Xm_train)           # h:[N,128], u:[N,2]\n",
    "    h_train = h_train.cpu().numpy()\n",
    "    u_train = u_train.cpu().numpy()\n",
    "\n",
    "print(\"[done] embeddings ready:\",\n",
    "      f\"h_train {h_train.shape}, u_train {u_train.shape}\")"
   ],
   "id": "2a8dd5ac5a014968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] embeddings ready: h_train (1000, 128), u_train (1000, 2)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T20:23:44.871055Z",
     "start_time": "2025-12-12T20:23:44.858684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save\n",
    "np.savez(\"./\", emb=u_train, coords=coords_np)"
   ],
   "id": "f5d37929be8d566f",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
